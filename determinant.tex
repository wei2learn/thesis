
\chapter{\label{chap:determinant}Diagonal Entries of Hermite Normal Form
and Determinant}

In this chapter, we consider the problem of computing the diagonal
entries of the Hermite normal form and the determinant of a nonsingular
input matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{n\times n}$
with column degrees $\vec{s}$. \citet{storjohann:2002,storjohann:2003}
gave an efficient Las Vegas for computing the determinant. Here we
give a efficient deterministic algorithm that costs $O^{\sim}\left(n^{\omega}s\right)$
field operations, where $s=\sum\vec{s}/n$ The computation is done
by using the column basis and kernel basis computation to compute
the diagonal entries of the Hermite form of $\mathbf{F}$, and then
multiply these diagonal entries.

Consider unimodularly transforming $\mathbf{F}$ to 
\begin{equation}
\mathbf{F}\mathbf{U}=\mathbf{G}=\begin{bmatrix}\mathbf{G}_{1} & 0\\
* & \mathbf{G}_{2}
\end{bmatrix},\label{eq:step1HermiteDiagonal}
\end{equation}


 After this unimodular transformation, which eliminated the top
right block of $\mathbf{G}$ , the matrix is now closer to the Hermite
normal form of $\mathbf{F}$. This procedure can then be applied recursively
to $\mathbf{G}_{1}$ and $\mathbf{G}_{2}$, until the matrices reaching
dimension 1, which then gives the diagonal entries of the Hermite
normal form of $\mathbf{F}$.

Although this procedure correctly computes the diagonal entries of
the Hermite normal form of $\mathbf{F}$, a major problem is that
the degree of the unimodular $\mathbf{U}$ can be too large for $\mathbf{U}$
to be efficiently computed. However, with the tools we have developed
in the earlier chapters, we can efficiently compute $\mathbf{G}_{1}$
and $\mathbf{G}_{2}$ without computing $\mathbf{U}$.

If we separate $\mathbf{F}$ to $\mathbf{F}=\begin{bmatrix}\mathbf{F}_{U}\\
\mathbf{F}_{D}
\end{bmatrix}$, each has full-rank as $\mathbf{F}$ is assumed to be nonsingular,
and also separate $\mathbf{U}$ to $\mathbf{U}=\begin{bmatrix}\mathbf{U}_{L} & \mathbf{U}_{R}\end{bmatrix}$,
where the column dimension of $\mathbf{U}_{L}$ matches the row dimension
of $\mathbf{F}_{U}$, then 
\[
\mathbf{F}\mathbf{U}=\begin{bmatrix}\mathbf{F}_{U}\\
\mathbf{F}_{D}
\end{bmatrix}\begin{bmatrix}\mathbf{U}_{L} & \mathbf{U}_{R}\end{bmatrix}=\mathbf{G}=\begin{bmatrix}\mathbf{G}_{1} & 0\\
* & \mathbf{G}_{2}
\end{bmatrix}.
\]
 Notice that the matrix $\mathbf{G}_{1}$ is nonsingular and is therefore
just a column basis of $\mathbf{F}_{U}$, and can be efficiently computed
using \prettyref{alg:colBasis}. To compute $\mathbf{G}_{2}=\mathbf{F}_{D}\mathbf{U}_{R}$,
notice that the matrix $\mathbf{U}_{R}$ is a right kernel basis\textbf{
}of $\mathbf{F}$, which makes the top right block of $\mathbf{G}$
zero. As we have seen from \prettyref{lem:unimodular_kernel_columnBasis},
the kernel basis $\mathbf{U}_{R}$ can be replaced by any other kernel
basis of $\mathbf{F}$ to give another unimodular matrix that also
works. 
\begin{lem}
\label{lem:oneStepHermiteDiagonal}Given a polynomial matrix $\mathbf{F}=\begin{bmatrix}\mathbf{F}_{U}\\
\mathbf{F}_{D}
\end{bmatrix}$. If $\mathbf{G}_{1}$ is a column basis of $\mathbf{F}_{U}$ and
$\mathbf{N}$ is a kernel basis of $\mathbf{F}_{U}$, then there is
a unimodular matrix $\mathbf{U}=\left[*,\mathbf{N}\right]$ such that
\[
\mathbf{F}\mathbf{U}=\begin{bmatrix}\mathbf{G}_{1}\\
* & \mathbf{G}_{2}
\end{bmatrix},
\]
 where $\mathbf{G}_{2}=\mathbf{F}_{D}\mathbf{N}$.  If $\mathbf{F}$
is square nonsingular, then $\mathbf{G}_{1}$ and $\mathbf{G}_{2}$
are also square nonsingular.
\end{lem}
Note that we do not compute the blocks represented by the symbol $*$,
which may have very large degrees and cannot be computed efficiently.

\prettyref{lem:oneStepHermiteDiagonal} allows us to compute $\mathbf{G}_{1}$
and $\mathbf{G}_{2}$ independently without computing the unimodular
matrix. $\mathbf{G}_{1}$ can be computed using the method from \prettyref{chap:Matrix-GCD},
while the kernel basis computation from \prettyref{chap:NullspaceBasis}
can be used to compute a kernel basis $\mathbf{N}$ of $\mathbf{F}_{U}$,
which can then be used to compute $\mathbf{G}_{2}=\mathbf{F}_{D}\mathbf{N}$.

After $\mathbf{G}_{1}$ and $\mathbf{G}_{2}$ are computed, we can
repeat the same process on each of these two matrices, which now have
lower dimensions, until the dimension becomes one. This procedure
of computing the diagonal entries gives \prettyref{alg:hermiteDiagonal}

\input{algorithmHermiteDiagonalEntries.tex}


\section{Computing the Determinant}

The product of the diagonal entries computed from \prettyref{alg:hermiteDiagonal}
is an associate of the determinant, that is, the product equals $c\det\mathbf{F}$
for some $c\in\mathbb{K}$, since the unimodular matrices that eliminate
the top right blocks may not have its determinant equal to 1. Therefore,
to get the determinant of $\mathbf{F}$, we need to scale the product
of the diagonal entries by $c^{-1}$, where $c$ is the determinant
of the unimodular matrix that transforms $\mathbf{F}$ to the diagonal
entries from the algorithm.


\begin{lem}
\label{lem:scalingToDeterminant}Let $\mathbf{U}=\left[\mathbf{U}_{L},\mathbf{U}_{R}\right]$
be a unimodular that eliminates the top right block of $\mathbf{F}$
as before, that is, 
\[
\mathbf{F}\mathbf{U}=\begin{bmatrix}\mathbf{F}_{U}\\
\mathbf{F}_{D}
\end{bmatrix}\begin{bmatrix}\mathbf{U}_{L} & \mathbf{U}_{R}\end{bmatrix}=\begin{bmatrix}\mathbf{G}_{1} & 0\\
* & \mathbf{G}_{2}
\end{bmatrix}=\mathbf{G}.
\]
 Let $\mathbf{V}=\begin{bmatrix}\mathbf{V}_{U}\\
\mathbf{V}_{D}
\end{bmatrix}=\mathbf{U}^{-1}$. Let $U_{R}=\mathbf{U}_{R}\mod x$, $V_{U}=\mathbf{V}_{U}\mod x$,
and $U_{L}^{*}\in\mathbb{K}^{n\times*}$ is a matrix that gives a
unimodular completion $U^{*}=\left[U_{L}^{*},U_{R}\right]$ of $U_{R}$
with $\det U^{*}=a\in\mathbb{K}$. Then $\det\mathbf{F}=\det\mathbf{G}\det\left(V_{U}U_{L}^{*}\right)/a$.\end{lem}
\begin{proof}
Since $\det\mathbf{F}=\det\mathbf{G}\det\mathbf{V}$, we just need
to show that $\det\mathbf{V}=\det\left(V_{U}U_{L}^{*}\right)/a$.
In fact, we just need to check the degree 0 coefficient matrix $V=\mathbf{V}\mod x$
to show that $\det V=\det\left(V_{U}U_{L}^{*}\right)/a$, since $\mathbf{V}$
is unimodular, which makes $\det V=\det\mathbf{V}$. Consider now
\begin{eqnarray*}
\det V\det U^{*} & = & \det\left(VU^{*}\right)\\
 & = & \det\left(\begin{bmatrix}V_{U}\\
V_{D}
\end{bmatrix}\begin{bmatrix}U_{L}^{*} & U_{R}\end{bmatrix}\right)\\
 & = & \det\left(\begin{bmatrix}V_{U}U_{L}^{*} & 0\\
* & I
\end{bmatrix}\right)\\
 & = & \det\left(V_{U}U_{L}^{*}\right),
\end{eqnarray*}
 hence $\det V=\det\left(V_{U}U_{L}^{*}\right)/a$.
\end{proof}
\prettyref{lem:scalingToDeterminant} requires us to compute a unimodular
completion of a matrix $U_{R}$, which is a very simple special case
of the unimodular completion from \prettyref{chap:Unimodular-Completion}
and can be obtained easily from the unimodular matrix that transforms
$V_{U}$ to its reduced column echelon form. Such unimodular matrix
can be computed using the Gauss Jordan transform algorithm from \citet{storjohann:phd2000}
on $V_{U}$ with a cost of $O\left(nm^{\omega-1}\right)$.

We can now compute the actual determinant of $\mathbf{F}$ by simply
computing the scaling factor at each step. The updated algorithm that
also computes the scaling factor is given in \prettyref{alg:hermiteDiagonalWithScalingFactor}.

\input{algorithmDeterminant.tex}




\section{Computational Cost}

Let us look at the computational cost of \prettyref{alg:hermiteDiagonal}
and \prettyref{alg:hermiteDiagonalWithScalingFactor}. 
\begin{thm}
\prettyref{alg:hermiteDiagonal} and \prettyref{alg:hermiteDiagonalWithScalingFactor}
cost $O^{\sim}\left(n^{\omega}s\right)$ field operations to compute
the diagonal entries for the Hermite normal form of a nonsingular
matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{n\times n}$, where
$s$ is the average column degree of $\mathbf{F}$. \end{thm}
\begin{proof}
The three main operations are computing a column basis of $\mathbf{F}_{U}$,
computing a kernel basis $\mathbf{N}$ of $\mathbf{F}_{U}$, and the
matrix multiplication $\mathbf{F}_{D}\mathbf{N}$.

For the column basis computation, by \prettyref{thm:columnBasisCost1}
we know that a column basis $\mathbf{G}_{1}$ of $\mathbf{F}_{U}$
can be computed with a cost of $O^{\sim}\left(n^{\omega}s\right).$
By \prettyref{lem:colBasisDegreeBoundByInputDegrees} the column degrees
of the computed column basis $\mathbf{G}_{1}$ are also bounded by
the original column degrees $\vec{s}$.

For the kernel basis computation, it also costs $O^{\sim}\left(n^{\omega}s\right)$
to compute a $\vec{s}$-minimal kernel basis $\mathbf{N}$ of $\mathbf{F}_{U}$
from \prettyref{thm:costLowColDimension}. The sum of the $\vec{s}$-column
degrees of the output kernel basis $\mathbf{N}$ is bounded by $\sum\vec{s}$
by \prettyref{thm:boundOfSumOfShiftedDegreesOfKernelBasis}.

Finally for the matrix multiplication $\mathbf{F}_{D}\mathbf{N}$,
since the sum of the column degrees of $\mathbf{F}_{D}$ and the sum
of the $\vec{s}$-column degrees of $\mathbf{N}$ are both bounded
by $\sum\vec{s}$, \prettyref{thm:multiplyUnbalancedMatrices} applies
and the multiplication can be done with a cost of $O^{\sim}\left(n^{\omega}s\right)$.

Now if we let the cost of \prettyref{alg:hermiteDiagonal} be $g(n)$
for a input matrix of dimension $n$, then we have the recurrence
relation
\[
g(n)\in O^{\sim}(n^{\omega-1})+g(\left\lceil n/2\right\rceil )+g(\left\lfloor n/2\right\rfloor ),
\]
 which is the same as in \prettyref{thm:inverseCost} for computing
the matrix inverse. Therefore, we also get $g(n)=O^{\sim}(n^{\omega}s)$
as in the inverse computation.

The only extra cost of \prettyref{alg:hermiteDiagonalWithScalingFactor}
is the cost unimodular completion of $U_{R}$, which is just $O\left(n^{\omega}\right)$.
So it has the same cost as \prettyref{alg:hermiteDiagonal}. \end{proof}
\begin{cor}
The determinant of a nonsingular matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{n\times n}$
can be computed with a cost of $O^{\sim}(n^{\omega}s)$ field operations,
where $s$ is the minimum of the average column degree and the average
row degree of the input matrix.\end{cor}
\begin{proof}
We can just use \prettyref{alg:hermiteDiagonalWithScalingFactor}
to compute the diagonal entries of the Hermite normal form of either
$\mathbf{F}$ or $\mathbf{F}^{T}$ with the scaling factor, and then
multiply the diagonal entries with the scaling factor.\end{proof}

