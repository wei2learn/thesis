
\chapter{\label{chap:Matrix-GCD}Column Basis}

In this Chapter, we consider the problem of computing a column basis
of an input matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
with $n\ge m$ and column degrees bounded by a shift $\vec{s}$. An
efficient way of computing a column basis immediately leads to efficient
computations of matrix GCDs, column reduced forms and Popov forms
of $\mathbf{F}$ with any dimension and rank. Note that the existing
algorithms from \citet{Giorgi2003}, \citet{GSSV2012},\citet{beelen:1988,sarkar2011}
\citet{SS2011}, and \citet{sarkar2011,bcl:2006} \citet{sarkar2011}for
computing column reduced forms and Popov forms require that the input
matrices be square nonsingular or full-column rank. As in the previous
chapters, $\mathbf{F}$ will always be our input matrix in this chapter,
and $\vec{s}$ will be a shift with entries that bound the column
degrees of $\mathbf{F}$.

Column basis is a generalization of polynomial GCDs. Recall that a
column basis $\mathbf{T}\in\mathbb{K}\left[x\right]^{m\times r}$
of $\mathbf{F}$ is a full rank matrix that generates the same $\mathbb{F}\left[x\right]$-module
as generated by the columns of $\mathbf{F}$. From \prettyref{cor:unimodularlyReduceToColumnBasis},
we know any matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
can be unimodularly transformed to a column basis by repeatedly working
with the leading column coefficient matrices. But computing a column
basis this way can be expensive, as we need to work with up to $\xi=\sum\vec{s}$
such coefficient matrices, which would involve up to $\xi$ polynomial
matrix multiplications.  Before discussing the efficient computation
of column basis, it is useful to look at following relationship between
column basis, kernel basis, and unimodular matrix.
\begin{lem}
\label{lem:unimodular_kernel_columnBasis}Given $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$.
If $\mathbf{U}$ is a unimodular matrix such that $\mathbf{F}\mathbf{U}=\left[0,\mathbf{T}\right]$
gives a full column rank $\mathbf{T}$, then $\mathbf{U}$ can be
separated into two submatrices $\mathbf{U}=\left[\mathbf{U}_{L},\mathbf{U}_{R}\right]$,
where $\mathbf{U}_{L}$ is a kernel basis of $\mathbf{F}$ and $\mathbf{F}\mathbf{U}_{R}=\mathbf{T}$
is a column basis of $\mathbf{F}$. In addition, the kernel basis
$\mathbf{U}_{L}$ can be replaced by any other kernel basis $\mathbf{N}$
of $\mathbf{F}$ and still gives a unimodular matrix $\left[\mathbf{N},\mathbf{U}_{R}\right]$,
which can also be used to unimodularly transform $\mathbf{F}$ to
$\left[0,\mathbf{T}\right]$. \end{lem}
\begin{proof}
Note that $\mathbf{T}$ is a column basis of $\mathbf{F}$ by \prettyref{cor:unimodularlyReduceToColumnBasis}.
It remains to show that $\mathbf{U}_{L}$ is a kernel basis of $\mathbf{F}$.
Since $\mathbf{F}\mathbf{U}_{L}=0$, $\mathbf{U}_{L}$ is generated
by any kernel basis $\mathbf{N}$, that is, $\mathbf{U}_{L}=\mathbf{N}\mathbf{C}$
for some polynomial matrix $\mathbf{C}$. Let $r$ be the rank of
$\mathbf{F}$, which is also the column dimension of $\mathbf{T}$
and $\mathbf{U}_{R}$. Then both $\mathbf{N}$ and $\mathbf{U}_{L}$
have column dimension $n-r$. Hence $\mathbf{C}$ is a square $(n-r)\times(n-r)$
matrix. Now the unimodular matrix $\mathbf{U}$ can be factored as
\[
\mathbf{U}=\left[\mathbf{N}\mathbf{C},\mathbf{U}_{R}\right]=\left[\mathbf{N},\mathbf{U}_{R}\right]\begin{bmatrix}\mathbf{C}\\
 & I
\end{bmatrix},
\]
 implying that both factors $\left[\mathbf{N},\mathbf{U}_{R}\right]$
and $\begin{bmatrix}\mathbf{C}\\
 & I
\end{bmatrix}$ are unimodular. Therefore, $\mathbf{C}$ is unimodular and $\mathbf{U}_{L}=\mathbf{N}\mathbf{C}$
is also a kernel basis. Notice that the unimodular matrix $\left[\mathbf{N},\mathbf{U}_{R}\right]$
also transforms $\mathbf{F}$ to $\left[0,\mathbf{T}\right]$.
\end{proof}
\prettyref{lem:unimodular_kernel_columnBasis} gives the following
result for a unimodular matrix and its inverse.
\begin{cor}
\label{cor:unimodular_kernel_columnBasis2}Let $\mathbf{U}=\left[\mathbf{U}_{L},\mathbf{U}_{R}\right]$
be any unimodular matrix with columns separated arbitrarily to $\mathbf{U}_{L}$
and $\mathbf{U}_{R}$. Let its inverse $\mathbf{V}=\begin{bmatrix}\mathbf{V}_{U}\\
\mathbf{V}_{D}
\end{bmatrix}$, where the row dimension of $\mathbf{V}_{U}$ matches the column
dimension of $\mathbf{U}_{L}$. So we have 
\[
\mathbf{V}\mathbf{U}=\begin{bmatrix}\mathbf{V}_{U}\\
\mathbf{V}_{D}
\end{bmatrix}\begin{bmatrix}\mathbf{U}_{L},\mathbf{U}_{R}\end{bmatrix}=\begin{bmatrix}\mathbf{V}_{U}\mathbf{U}_{L} & \mathbf{V}_{U}\mathbf{U}_{R}\\
\mathbf{V}_{D}\mathbf{U}_{L} & \mathbf{V}_{D}\mathbf{U}_{R}
\end{bmatrix}=\begin{bmatrix}I & 0\\
0 & I
\end{bmatrix}.
\]
Then $\mathbf{V}_{U}\mathbf{U}_{L}=I$ is a column basis of $\mathbf{V}_{U}$
and a row basis of $\mathbf{U}_{L}$, while $\mathbf{V}_{D}\mathbf{U}_{R}=I$
is a column basis of $\mathbf{V}_{D}$ and a row basis of of $\mathbf{U}_{R}$.
In addition, \textup{$\mathbf{V}_{D}$ and $\mathbf{U}_{L}$ are kernel
bases of each other, while $\mathbf{V}_{U}$ and $\mathbf{U}_{R}$
are kernel bases of each other.}\end{cor}
\begin{proof}
This follows directly from \prettyref{lem:unimodular_kernel_columnBasis},
by taking $\mathbf{F}$ from \prettyref{lem:unimodular_kernel_columnBasis}
to be $\mathbf{V}_{U}$, $\mathbf{V}_{D}$, $\mathbf{U}_{L}^{T}$,
and $\mathbf{U}_{R}^{T}$ here.
\end{proof}
To compute a column basis of $\mathbf{F}$, we use the following procedure.
We first compute a right  kernel basis $\mathbf{N}$ of $\mathbf{F}$.
Then we compute a left  kernel basis $\mathbf{G}$ of $\mathbf{N}$.
This matrix $\mathbf{G}$ is a right factor of $\mathbf{F}$, that
is, $\mathbf{F}=\mathbf{T}\mathbf{G}$ for some $\mathbf{T}\in\mathbb{K}\left[x\right]^{m\times r}$.
Then we can compute the left factor $\mathbf{T}$, which is in fact
a column basis of $\mathbf{F}$.
\begin{lem}
\label{lem:matrixGCD}Given $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$%
\begin{comment}
 and has full row rank
\end{comment}
. Let $\mathbf{N}\in\mathbb{K}\left[x\right]^{n\times(n-r)}$ be any
right kernel basis of $\mathbf{F}$, and $\mathbf{G}\in\mathbb{K}\left[x\right]^{r\times n}$
be any left kernel basis of $\mathbf{N}$, where $r$ is the rank
of $\mathbf{F}$. Then $\mathbf{F}=\mathbf{T}\mathbf{G}$ for some
$\mathbf{T}\in\mathbb{K}\left[x\right]^{m\times r}$ and $\mathbf{T}$
is a column basis of $\mathbf{F}$.\end{lem}
\begin{proof}
Let the matrix $\mathbf{U}=\begin{bmatrix}\mathbf{U}_{L},\mathbf{U}_{R}\end{bmatrix}$
from \prettyref{cor:unimodular_kernel_columnBasis2} be a unimodular
matrix that transforms $\mathbf{F}$ to a column basis $\mathbf{B}\in\mathbb{K}\left[x\right]^{m\times r}$
of $\mathbf{F}$, where $\mathbf{U}_{L}$ is any right kernel basis
$\mathbf{F}$. From $\mathbf{F}\mathbf{U}=\left[0,\mathbf{B}\right]$,
we get\textbf{ $\mathbf{F}=\left[0,\mathbf{B}\right]\mathbf{U}^{-1}=\mathbf{B}\left[0,I\right]\mathbf{V}=\mathbf{B}\mathbf{V}_{D}$}.
Since $\mathbf{V}_{D}$ is a left kernel basis of\textbf{ $\mathbf{U}_{L}$},
any other left kernel basis $\mathbf{G}$ of $\mathbf{U}_{L}$ is
unimodularly equivalent to $\mathbf{V}_{D}$, that is, $\mathbf{V}_{D}=\mathbf{W}\mathbf{G}$
for some unimodular matrix $\mathbf{W}$. Now $\mathbf{F}=\mathbf{B}\mathbf{W}\mathbf{G}$,
where $\mathbf{BW}=\mathbf{T}$ a column basis of $\mathbf{F}$ since
it is unimodularly equivalent to the column basis $\mathbf{B}$.
\end{proof}


\prettyref{lem:matrixGCD} outlines a procedure for computing a column
basis of $\mathbf{F}$ with three main steps. The first step is to
compute a $\left(\mathbf{F},\vec{s}\right)$-kernel basis $\mathbf{N}$,
which can be efficiently done using \prettyref{alg:minimalNullspaceBasisWithRankProfile}.
However, we still need to work on the second step of computing a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{G}^{T}$ and the third step of computing the column
basis $\mathbf{T}$ from $\mathbf{F}$ and $\mathbf{G}$. Note that
while \prettyref{lem:matrixGCD} does not require the bases computed
to be minimal, working with minimal bases keeps the degrees well-managed
and helps to make the computation efficient.


\section{\label{sec:computeRightFactor}Computing a Right Factor}

Let us now look at the computation of a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{G}^{T}$. For this problem, \prettyref{alg:minimalNullspaceBasisWithRankProfile}
does not work well directly, since the input matrix $\mathbf{N}^{T}$
has nonuniform row degrees and negative shift. Comparing to the earlier
problem of computing a $\left(\mathbf{F},\vec{s}\right)$-kernel basis
$\mathbf{N}$, it is interesting to note that the old output $\mathbf{N}$
now becomes the new input matrix $\mathbf{N}^{T}$, while the new
output matrix $\mathbf{G}$ has size bounded by $\mathbf{F}$. In
other words, the new input has degrees that matches the old output,
while the new output has degrees bounded by the old input. It is
therefore reasonable to expect that the new problem can be computed
efficiently. However, we need to find some way to work with the more
complicated input degree structure. On the other hand, the simpler
output degree structure makes it easier to apply order basis computation
to compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis. 

To see how order basis computations can be applied here, let us first
extend \prettyref{lem:orderBasisContainsNullspaceBasis}, which provides
a relationship between order bases and kernel bases, to accommodate
our situation here.
\begin{lem}
\label{lem:orderbasisContainsNullspacebasisGeneralized}Given a matrix
$\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$ and a degree
shift $\vec{u}$ with $\rdeg_{\vec{u}}\mathbf{A}\le\vec{v}$, or equivalently,
$\cdeg_{-\vec{v}}\mathbf{A}\le-\vec{u}$. Let $\mathbf{P}=\left[\mathbf{P}_{1},\mathbf{P}_{2}\right]$
be any $\left(\mathbf{A},\vec{v}+1,-\vec{u}\right)$-basis and $\mathbf{Q}=\left[\mathbf{Q}_{1},\mathbf{Q}_{2}\right]$
be any $(\mathbf{A},-\vec{u})$-kernel basis, where $\mathbf{P}_{1}$
and $\mathbf{Q}_{1}$ contain all columns from $\mathbf{P}$ and $\mathbf{Q}$,
respectively, whose $-\vec{u}$-column degrees are no more than $0$.
Then $\left[\mathbf{P}_{1},\mathbf{Q}_{2}\right]$ is an $(\mathbf{A},-\vec{u})$-kernel
basis, and $\left[\mathbf{Q}_{1},\mathbf{P}_{2}\right]$ is an $\left(\mathbf{A},\vec{v}+\left[1,\dots,1\right],-\vec{u}\right)$-basis.\end{lem}
\begin{proof}
We know $\cdeg_{-\vec{v}}\mathbf{A}\mathbf{P}_{1}\le\cdeg_{-\vec{u}}\mathbf{P}_{1}\le0$,
or equivalently, $\rdeg\mathbf{A}\mathbf{P}_{1}\le\vec{v}$, but it
has order greater than $\vec{v}$, hence $\mathbf{A}\mathbf{P}_{1}=0$.
The result then follows the same reasoning as in the proof of \prettyref{lem:orderBasisContainsNullspaceBasis}.%
\begin{comment}
We know $\cdeg\mathbf{P}_{1}^{T}\le\vec{u}$ from $\cdeg_{-\vec{u}}\mathbf{P}_{1}\le0$,
hence $\cdeg\mathbf{P}_{1}^{T}\mathbf{A}^{T}\le\cdeg_{\vec{u}}\mathbf{A}^{T}$
by \prettyref{lem:boundOnDegreesOfFA}. Now for each row $\mathbf{a}_{i}$
in $\mathbf{A}$ and its $\vec{u}$-column degree $v_{i}$, we have
\[
\rdeg\mathbf{a}_{i}\mathbf{P}_{1}=\cdeg\mathbf{P}_{1}^{T}\mathbf{a}_{i}^{T}\le\cdeg_{\vec{u}}\mathbf{a}_{i}^{T}=v_{i},
\]
 and $\mathbf{a}_{i}\mathbf{P}_{1}\equiv0\mod x^{v_{i}+1}$, hence
$\mathbf{A}\mathbf{P}_{1}=0$. The result then follows the same reasoning
as in the proof of \prettyref{lem:orderBasisContainsNullspaceBasis}.
\end{comment}

\end{proof}
Now with the help of \prettyref{lem:orderbasisContainsNullspacebasisGeneralized},
let us get back to our problem of computing a $(\mathbf{F},\vec{s})$-kernel
basis.  In fact, we just need to use a special case of \prettyref{lem:orderbasisContainsNullspacebasisGeneralized},
where all the elements of the kernel basis have shifted degrees bounded
by $0$, making the partial kernel basis a complete kernel basis%
\begin{comment}
, which follows from our requirement of using a shift $\vec{s}\ge\cdeg\mathbf{F}$
\end{comment}
.
\begin{lem}
\label{lem:nullspaceBasisInOrderBasis}Let $\mathbf{N}$ be a $(\mathbf{F},\vec{s})$-kernel
basis with $\cdeg_{\vec{s}}\mathbf{N}=\vec{b}$. Let $\mathbf{P}=\left[\mathbf{P}_{1},\mathbf{P}_{2}\right]$
be a $\left(\mathbf{N}^{T},\vec{b}+1,-\vec{s}\right)$-basis, where
$\mathbf{P}_{1}$ consists of all columns $\mathbf{p}$ with $\cdeg_{-\vec{s}}\mathbf{p}\le0$.
\begin{comment}
of $\mathbf{P}$ satisfying $\mathbf{N}^{T}\mathbf{p}=0$. 
\end{comment}
Then $\mathbf{P}_{1}$ is a $(\mathbf{N}^{T},-\vec{s})$-kernel basis. \end{lem}
\begin{proof}
Let the rank of $\mathbf{F}$ be $r$, which is also the column dimension
of any $(\mathbf{N}^{T},-\vec{s})$-kernel basis $\mathbf{G}^{T}$.
Since both $\mathbf{F}$ and $\mathbf{G}$ are in the left kernel
of $\mathbf{N}$, we know $\mathbf{F}$ is generated by $\mathbf{G}$,
and the $-\vec{s}$-row degrees of $\mathbf{G}$ are bounded by the
corresponding $r$ largest $-\vec{s}$-row degrees of $\mathbf{F}$,
which are in turn bounded by $0$ since $\cdeg\mathbf{F}\le\vec{s}$.
Therefore, any $(\mathbf{N}^{T},-\vec{s})$-kernel basis $\mathbf{G}^{T}$
satisfies $\cdeg_{-\vec{s}}\mathbf{G}^{T}\le0$. The result now follows
from \prettyref{lem:orderbasisContainsNullspacebasisGeneralized}.
\end{proof}
We can use \prettyref{thm:continueComputingNullspaceBasisByRows}
to compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis
by rows. If we separate $\mathbf{N}$ to $\mathbf{N}=\left[\mathbf{N}_{1},\mathbf{N}_{2}\right]$
with $\vec{s}$-column degrees $\vec{b}_{1}$, $\vec{b}_{2}$ respectively,
and first compute a $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{Q}_{1}$ with $-\vec{s}$-column degrees $-\vec{s}_{2}$,
and then compute a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis $\mathbf{Q}_{2}$, then $\mathbf{Q}_{1}\mathbf{Q}_{2}$ is a
$\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis. To compute kernel
bases $\mathbf{Q}_{1}$ and $\mathbf{Q}_{2}$, we can use order basis
computation. However, we need to make sure that the order bases we
compute do contain these kernel bases.
\begin{lem}
\label{lem:nullspaceBasisOfSubsetOfRowsContainedInOrderBasis}Let
$\mathbf{N}=\left[\mathbf{N}_{1},\mathbf{N}_{2}\right]$, with $\vec{s}$-column
degrees $\vec{b}_{1}$, $\vec{b}_{2}$ respectively. Then a $\left(\mathbf{N}_{1}^{T},\vec{b}_{1}+1,-\vec{s}\right)$-basis
contains a $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel basis
whose $-\vec{s}$-row degrees are bounded by 0. Let $\mathbf{Q}_{1}$
be this kernel basis, and $-\vec{s}_{2}=\cdeg_{-\vec{s}}\mathbf{Q}_{1}$.
Then a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},\vec{b}_{2}+1,-\vec{s}_{2}\right)$-basis
contains a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis $\mathbf{Q}_{2}$ whose $-\vec{s}$-row degrees are bounded
by 0. The product $\mathbf{Q}_{1}\mathbf{Q}_{2}$ is then a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis.\end{lem}
\begin{proof}
To see that a $\left(\mathbf{N}_{1}^{T},\vec{b}_{1}+1,-\vec{s}\right)$-basis
contains a $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel basis
whose $-\vec{s}$-row degrees are bounded by 0, we just need to show
that $\rdeg_{-\vec{s}}\mathbf{\bar{Q}}_{1}\le0$ for any $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{\bar{Q}}_{1}$ and then apply \prettyref{lem:orderbasisContainsNullspacebasisGeneralized}.
Note that there exists a polynomial matrix $\bar{\mathbf{Q}}_{2}$
such that $\mathbf{\bar{Q}}_{1}\mathbf{\bar{Q}}_{2}=\bar{\mathbf{G}}$
for any $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis $\bar{\mathbf{G}}$,
as $\bar{\mathbf{G}}$ satisfies $\mathbf{N}_{1}^{T}\bar{\mathbf{G}}=0$
and is therefore generated by the $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis $\bar{\mathbf{Q}}_{1}$. If $\rdeg_{-\vec{s}}\mathbf{\bar{Q}}_{1}\nleq0$,
then \prettyref{lem:predictableDegree} forces $\rdeg_{-\vec{s}}\left(\bar{\mathbf{Q}}_{1}\bar{\mathbf{Q}}_{2}\right)=\rdeg_{-\vec{s}}\bar{\mathbf{G}}\nleq0$,
a contradiction. 

As before, to see that a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},\vec{b}_{2}+1,-\vec{s}_{2}\right)$-basis
contains a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis whose $-\vec{s}$-row degrees are no more than 0, we can just
show $\rdeg_{-\vec{s}_{2}}\hat{\mathbf{Q}}_{2}\le0$ for any $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis $\hat{\mathbf{Q}}_{2}$ and then apply \prettyref{lem:orderbasisContainsNullspacebasisGeneralized}.
Note that $\rdeg_{-\vec{b}_{2}}\mathbf{N}_{2}^{T}\mathbf{Q}_{1}\le-\vec{s}_{2}$.
Let $\hat{\mathbf{G}}=\mathbf{Q}_{1}\hat{\mathbf{Q}}_{2}$, a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis. Note that $\rdeg_{-\vec{s}_{2}}\hat{\mathbf{Q}}_{2}=\rdeg_{-\vec{s}}\mathbf{Q}_{1}\hat{\mathbf{Q}}_{2}=\rdeg_{-\vec{s}}\hat{\mathbf{G}}\le0$. 
\end{proof}
Now that we can correctly compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis by rows with the help of order basis computation, we need to
look at how to do it efficiently. One major difficulty is that the
order, or equivalently, the $\vec{s}$-row degrees are nonuniform
and can have degree as large as $\xi=\sum\vec{s}$. To overcome this,
we separate the rows of $\mathbf{N}^{T}$ into blocks according to
their $\vec{s}$-row degrees, and then work with these blocks one
by one successively using \prettyref{thm:continueComputingNullspaceBasisByRows}. 

\input{algorithmNullspaceBasisReverse.tex}

Let $k$ be the column dimension of $\mathbf{N}$. Since $\sum\cdeg_{\vec{s}}\mathbf{N}=\sum\vec{b}\le\sum\vec{s}\le\xi$,
at most $k/c$ columns of $\mathbf{N}$ have $\vec{s}$-column degrees
greater than or equal to $c\xi/k$ for any $c\ge1$. We assume without
loss of generality that the rows of $\mathbf{N}^{T}$ are arranged
in decreasing $\vec{s}$-row degrees. We divide $\mathbf{N}^{T}$
into $\log k$ row blocks according to the $\vec{s}$-row degrees
of its rows, or equivalently, divide $\mathbf{N}$ to blocks of columns
according to the $\vec{s}$-column degrees. Let 
\[
\mathbf{N}=\left[\mathbf{N}_{1},\mathbf{N}_{2},\cdots,\mathbf{N}_{\log k-1},\mathbf{N}_{\log k}\right]
\]
with $\mathbf{N}_{\log k},\mathbf{N}_{\log k-1},\dots,\mathbf{N}_{2},\mathbf{N}_{1}$
having $\vec{s}$-column degrees in the range $\left[0,2\xi/k\right]$,
$(2\xi/k,4\xi/k],$ $(4\xi/k,8\xi/k],\ ...,$ $(\xi/4,\xi/2],$ $(\xi/2,\xi].$
Let $\vec{\sigma}_{i}=\left[\xi/2^{i-1}+1,\dots,\xi/2^{i-1}+1\right]$
with the same dimension as the row dimension of $\mathbf{N}_{i}$.
Let $\vec{\sigma}=\left[\vec{\sigma}_{\log k},\vec{\sigma}_{\log k-1},\dots,\vec{\sigma}_{1}\right]$
be the order in the order basis computation.

To further simply our task, we also make the order of our problem
in each block uniform. Rather than of using $\mathbf{N}^{T}$ as the
input matrix, we use 
\begin{eqnarray*}
\hat{\mathbf{N}} & =\begin{bmatrix}\hat{\mathbf{N}}_{1}\\
\vdots\\
\hat{\mathbf{N}}_{\log k}
\end{bmatrix}= & x^{\vec{\sigma}-\vec{b}-1}\begin{bmatrix}\mathbf{N}_{1}^{T}\\
\vdots\\
\mathbf{N}_{\log k}^{T}
\end{bmatrix}=x^{\vec{\sigma}-\vec{b}-1}\mathbf{N}^{T}
\end{eqnarray*}
 instead, so that a $\left(\hat{\mathbf{N}},\vec{\sigma},-\vec{s}\right)$-basis
is a $\left(\mathbf{N}^{T},\vec{b}+1,-\vec{s}\right)$-basis.

We are now ready to compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis, which is done by a series of order basis computations that
computes a series of kernel bases as follows.

Let $\vec{s}_{1}=\vec{s}$. First we compute an $\left(\hat{\mathbf{N}}_{1},\vec{\sigma}_{1},-\vec{s}_{1}\right)$-basis
$\mathbf{P}_{1}=\left[\mathbf{G}_{1},\mathbf{Q}_{1}\right]$, where
$\mathbf{G}_{1}$ is a $\left(\hat{\mathbf{N}}_{1},-\vec{s}_{1}\right)$-kernel
basis%
\begin{comment}
 with $\cdeg_{-\vec{s}_{1}}\mathbf{N}_{1}\le0$
\end{comment}
.

Let $\tilde{\mathbf{G}}_{1}=\mathbf{G}_{1}$. Let $\vec{s}_{2}=-\cdeg_{-\vec{s}}\mathbf{G}_{1}$.
\begin{comment}
Note that $-\vec{s}_{1}\le-[\vec{s}_{2},\vec{t}_{2}]\le\left[0,\dots,0,1,\dots1\right]$
component-wise, since $\mathbf{P}_{1}$ has lower order than any $\left(\mathbf{M}^{T},\vec{b}+\left[1,\dots,1\right],-\vec{s}\right)$-basis
$\mathbf{P}$ hence generates $\mathbf{P}$. Therefore, $\cdeg_{-\vec{s}}\mathbf{P}_{1}\le\cdeg_{-\vec{s}}\mathbf{P}\le\left[0,\dots,0,1,\dots1\right]$. 
\end{comment}
{} We then compute an $\left(\hat{\mathbf{N}}_{2}\tilde{\mathbf{G}}_{1},\vec{\sigma}_{2},-\vec{s}_{2}\right)$-basis
$\mathbf{P}_{2}=\left[\mathbf{G}_{2},\mathbf{Q}_{2}\right]$ with
$\vec{s}_{3}=-\cdeg_{-\vec{s}_{2}}\mathbf{G}_{2}$. Let $\tilde{\mathbf{G}}_{2}=\tilde{\mathbf{G}}_{1}\mathbf{G}_{2}$.
 %
\begin{comment}
Let $\mathbf{R}_{1}=\left[\mathbf{N}_{1}\mathbf{Q}_{2},\mathbf{Q}_{1}\right]$
and $\mathbf{R}_{1}^{r}=\revCol\left(\mathbf{R}_{1},-\vec{s},\cdeg_{-\vec{s}}\mathbf{R}_{1}\right)$.
Then from \prettyref{lem:unimodularComputationByRows} we know $\left[\mathbf{F}^{T},\mathbf{R}_{1}^{r}\right]$
is a unimodular matrix.
\end{comment}


Continuing this process, at step $i$ we compute an $\left(\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1},\vec{\sigma}_{i},-\vec{s}_{i}\right)$-basis
$\mathbf{P}_{i}=\left[\mathbf{G}_{i},\mathbf{Q}_{i}\right]$. Let
$\tilde{\mathbf{G}}_{i}=\prod_{j=1}^{i}\mathbf{G}_{i}=\tilde{\mathbf{G}}_{i-1}\mathbf{G}_{i}$.
Note that $\tilde{\mathbf{G}}_{\log k}$ is a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis. 

This process of computing a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis gives \prettyref{alg:minimalNullspaceBasisReverse}.

Now let us check the cost of this algorithm. The cost is dominated
by the order basis computation and the multiplications $\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1}$
and $\tilde{\mathbf{G}}_{i-1}\mathbf{G}_{i}$.
\begin{lem}
An $\left(\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1},\vec{\sigma}_{i},-\vec{s}_{i}\right)$-basis
can be computed at a cost of $O^{\sim}\left(n^{\omega}d\right)$,
where $d=\xi/n$.\end{lem}
\begin{proof}
Note that $\mathbf{N}_{i}$ has less than $2^{i}$ columns. Otherwise,
$\sum\cdeg_{\vec{s}}\mathbf{N}_{i}>2^{i}\xi/2^{i}=\xi$, contradicting
with $\sum\cdeg_{\vec{s}}\mathbf{N}=\sum\vec{b}\le\sum\vec{s}\le\xi$.
It follows that $\hat{\mathbf{N}}_{i}$, and therefore $\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1}$,
also have less than $2^{i}$ rows. We also have $\vec{\sigma}_{i}=\left[\xi/2^{i-1}+1,\dots,\xi/2^{i-1}+1\right]=\Theta\left(\xi/2^{i}\right)$.
Therefore, \prettyref{alg:umab} can be used with a cost of $O^{\sim}\left(n^{\omega}d\right)$
by \prettyref{thm:unbalancedOrderBasisCost}.\end{proof}
\begin{lem}
The multiplications $\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1}$
can be done with a cost of $O^{\sim}\left(n^{\omega}d\right)$.\end{lem}
\begin{proof}
The dimension of $\hat{\mathbf{N}}_{i}$ is bounded by $2^{i-1}\times n$
and $\sum\rdeg_{\vec{s}}\hat{\mathbf{N}}_{i}\le2^{i-1}\cdot\xi/2^{i-1}=\xi$.
We also have $\cdeg_{-\vec{s}}\tilde{\mathbf{G}}_{i-1}\le0$, or equivalently,
$\rdeg\tilde{\mathbf{G}}_{i-1}\le\vec{s}$. We can now use \prettyref{thm:multiplyUnbalancedMatrices}
to multiply $\tilde{\mathbf{G}}_{i-1}^{T}$ and $\hat{\mathbf{N}}_{i}^{T}$
with a cost of $O^{\sim}\left(n^{\omega-1}\xi\right)=O^{\sim}\left(n^{\omega}d\right)$.\end{proof}
\begin{lem}
The multiplication $\tilde{\mathbf{G}}_{i-1}\mathbf{G}_{i}$ can be
done with a cost of $O^{\sim}\left(n^{\omega}d\right)$.\end{lem}
\begin{proof}
We know $\cdeg_{-\vec{s}}\tilde{\mathbf{G}}_{i-1}=-\vec{s}_{i}$,
and $\cdeg_{-\vec{s}_{i}}\mathbf{G}_{i}=-\vec{s}_{i+1}\le0.$ In other
words, $\rdeg\mathbf{G}_{i}\le\vec{s}_{i}$, and $\rdeg_{\vec{s}_{i}}\tilde{\mathbf{G}}_{i-1}\le\vec{s}$,
hence we can again use \prettyref{thm:multiplyUnbalancedMatrices}
to multiply $\mathbf{G}_{i}^{T}$ and $\tilde{\mathbf{G}}_{i-1}^{T}$
with a cost of $O^{\sim}\left(n^{\omega}d\right)$.\end{proof}
\begin{thm}
A right factor $\mathbf{G}$ satisfying $\mathbf{TG}=\mathbf{F}$
for a column basis $\mathbf{T}$ can be computed with a cost of $O^{\sim}\left(n^{\omega-1}\xi\right)=O^{\sim}\left(n^{\omega}d\right)$.
\end{thm}

\section{Computing a Column Basis}

With a right factor $\mathbf{G}$ of $\mathbf{F}$ computed, we are
now ready to compute a column basis $\mathbf{T}$, where $\mathbf{F}=\mathbf{T}\mathbf{G}$.
To do so efficiently, the degree of $\mathbf{T}$ cannot be too big,
which is indeed the case as shown by the following lemmas.
\begin{lem}
\label{lem:colBasisdegreeBoundByRdegOfRightFactor}The column degrees
of $\mathbf{T}$ are bounded by the corresponding entries of $\vec{t}=-\rdeg_{-\vec{s}}\mathbf{G}$.\end{lem}
\begin{proof}
Since $\mathbf{G}$ is $-\vec{s}$-row reduced, and $\rdeg_{-\vec{s}}\mathbf{F}\le0$,
by \prettyref{lem:predictableDegree} $\rdeg_{-\vec{t}}\mathbf{T}\le0$,
or equivalently, $\mathbf{T}$ has column degrees bounded by $\vec{t}$.\end{proof}
\begin{lem}
\label{lem:colBasisDegreeBoundByInputDegrees}Let $\vec{t}=-\rdeg_{-\vec{s}}\mathbf{G}$,
a vector with $r$ entries and bounds $\cdeg\mathbf{T}$ from \prettyref{lem:colBasisdegreeBoundByRdegOfRightFactor}.
Let $\vec{s}'$ be the list of the $r$ largest entries of $\vec{s}$.
Then $\vec{t}\le\vec{s}'$.\end{lem}
\begin{proof}
Let $\mathbf{G}'$ be the $-\vec{s}$-row Popov form of $\mathbf{G}$,
and the square matrix $\mathbf{G}"$ consists of only the columns
of $\mathbf{G}'$ that contains pivot entries, and has the rows permuted
so the pivots are in the diagonal. Let $\vec{s}"$ be the list of
the entries in $\vec{s}$ that correspond to the columns of $\mathbf{G}"$
in $\mathbf{G}'$. Note that $\rdeg_{-\vec{s}"}\mathbf{G}"=-\vec{t}"$
is just a permutation of $-\vec{t}$ with the same entries. By the
definition of shifted row degree, $-\vec{t}"$ is the sum of $-\vec{s}"$
and the list of the diagonal pivot degrees, which are nonnegative.
Therefore, $-\vec{t}"\ge-\vec{s}"$. The result then follows as $\vec{t}$
is a permutation of $\vec{t}"$ and $\vec{s}'$ has the largest entries
of $\vec{s}$.
\end{proof}
With the bound on the column degrees of $\mathbf{T}$ determined,
we are now ready to compute $\mathbf{T}$. This is done again using
an order basis computation.
\begin{lem}
Let $\vec{t}'=\left[0,\dots,0,\vec{t}\right]\in\mathbb{Z}^{m+r}$.
Any $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis has the form $\begin{bmatrix}V\\
\bar{\mathbf{T}}
\end{bmatrix}$, where $V\in\mathbb{K}^{m\times m}$ is a unimodular matrix and $\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$
is a column basis of $\mathbf{F}$.\end{lem}
\begin{proof}
First, the matrix $\begin{bmatrix}I\\
\mathbf{T}^{T}
\end{bmatrix}$ is a kernel basis of $\left[\mathbf{F}^{T},\mathbf{G}^{T}\right]$
and is unimodularly equivalent to any other kernel basis. Hence we
must have $\begin{bmatrix}V\\
\bar{\mathbf{T}}
\end{bmatrix}=\begin{bmatrix}I\\
\mathbf{T}^{T}
\end{bmatrix}V$, which gives $\mathbf{T}=\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$.
Also note that the $-\vec{t}'$-minimality forces the unimodular matrix
$V$ in any $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis to be the same degree as $I$.
\end{proof}
To compute a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis, we can again use order basis computation.
\begin{lem}
Any $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],\vec{s}+\left[1,\dots,1\right],-\vec{t}'\right)$-basis
contain a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis whose $-\vec{t}'$-row degrees are bounded by 0.\end{lem}
\begin{proof}
As before, \prettyref{lem:orderbasisContainsNullspacebasisGeneralized}
can be used here. We just need to show that a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis has $-\vec{t}'$-row degrees no more than 0, which is true since
$\rdeg_{-\vec{t}'}\begin{bmatrix}I\\
\mathbf{T}^{T}
\end{bmatrix}\le0$.
\end{proof}
Now to compute a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis efficiently, notice we have the same type of problem as in \prettyref{sec:computeRightFactor},
and so \prettyref{alg:minimalNullspaceBasisReverse} works here as
well. With a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis $\begin{bmatrix}V\\
\bar{\mathbf{T}}
\end{bmatrix}$ computed, a column basis can now be easily computed by $\mathbf{T}=\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$.

We now have a complete algorithm for computing a column basis, given
in \prettyref{alg:colBasis}.

\input{algorithmColumnBasis.tex}
\begin{thm}
\label{thm:columnBasisCost1}A column basis $\mathbf{T}$ of $\mathbf{F}$
can be computed with a cost of $O^{\sim}\left(n^{\omega-1}\xi\right)$.
\end{thm}
\begin{comment}
This can be done by computing a left $\left[0,\dots,0,d,\dots,d\right]$-minimal
kernel basis $\left[\mathbf{T}',V\right]$ of $\left[\mathbf{G}^{T},\mathbf{F}^{T}\right]^{T}$,
where $d$ is the degree of $\mathbf{F},$ $V$ is a unimodular matrix
and $\mathbf{T}'$ is a $m\times m$ matrix. Note that $\left[\mathbf{T}',V\right]$
has $m$ rows since the rank of $\left[\mathbf{G}^{T},\mathbf{F}^{T}\right]^{T}$
is $m$. Also note that since $\left[\mathbf{T},I\right]$ is a left
kernel basis with $\left[0,\dots,0,d,\dots,d\right]$-row degrees
bounded by $d$, the $\left[0,\dots,0,d,\dots,d\right]$-minimal kernel
basis $\left[\mathbf{T}',V\right]$ must also has its $\left[0,\dots,0,d,\dots,d\right]$-row
degrees bounded by $d$, hence the degree of $V$ must be 0. We can
then easily compute $\mathbf{T}=\mathbf{T}'V^{-1}$. 
\end{comment}



\section{\label{sec:successiveColBasisComputation}A Simple Improvement}

When the input matrix $\mathbf{F}$ has column dimension much larger
$n$ than the row dimension $m$, we can separate $\mathbf{F}=\left[\mathbf{F}_{1},\mathbf{F}_{2},\dots,\mathbf{F}_{n/m}\right]$
to $n/m$ blocks, each with dimension $m\times m$, assuming without
loss of generality $n$ is a multiple of $m$, and the columns are
arranged in increasing degrees. We then do a series of column basis
computations. First we compute a column basis $\mathbf{T}_{1}$ of
$\left[\mathbf{F}_{1},\mathbf{F}_{2}\right]$. Then compute a column
basis $\mathbf{T}_{2}$ of $\left[\mathbf{T}_{1},\mathbf{F}_{3}\right]$.
Repeating this process, at step $i$, we compute a column basis $\mathbf{T}_{i}$
of $\left[\mathbf{T}_{i-1},\mathbf{F}_{i+1}\right]$, until $i=n/m-1$,
when a column basis of $\mathbf{F}$ is computed.
\begin{lem}
At step $i$, computing a column basis $\mathbf{T}_{i}$ of $\left[\mathbf{T}_{i-1},\mathbf{F}_{i+1}\right]$
can be done with a cost of $O^{\sim}\left(m^{\omega-1}\left(\sum\cdeg\mathbf{F}_{i}+\sum\cdeg\mathbf{F}_{i+1}\right)\right)$
field operations.\end{lem}
\begin{proof}
From \prettyref{lem:colBasisDegreeBoundByInputDegrees}, the column
basis $\mathbf{T}_{i-1}$ of $\left[\mathbf{F}_{1},\dots,\mathbf{F}_{i}\right]$
has column degrees bounded by the largest column degrees of $\mathbf{F}_{i}$,
hence $\sum\cdeg\mathbf{T}_{i-1}\le\sum\cdeg\mathbf{F}_{i}$. The
lemma then follows by combining this with the result that a column
basis $\mathbf{T}_{i}$ of $\left[\mathbf{T}_{i-1},\mathbf{F}_{i+1}\right]$
can be computed with a cost of $O^{\sim}\left(m^{\omega-1}\left(\sum\cdeg\mathbf{T}_{i-1}+\sum\cdeg\mathbf{F}_{i+1}\right)\right)$
from \prettyref{thm:columnBasisCost1}.\end{proof}
\begin{thm}
\label{thm:finalCollBasisCost}A column basis of $\mathbf{F}$ can
be computed with a cost of $O^{\sim}\left(m^{\omega-1}\xi\right)$.\end{thm}
\begin{proof}
Summing up the cost of all the column basis computations, 
\begin{eqnarray*}
 &  & \sum_{i=1}^{n/m-1}O^{\sim}\left(m^{\omega-1}\left(\sum\cdeg\mathbf{F}_{i}+\sum\cdeg\mathbf{F}_{i+1}\right)\right)\\
 & \subset & O^{\sim}\left(m^{\omega-1}\left(2\sum\cdeg\mathbf{F}\right)\right)\\
 & = & O^{\sim}\left(m^{\omega-1}\xi\right)
\end{eqnarray*}
\end{proof}
\begin{rem}
In this section, the computational efficiency is improved by reducing
the original problem to about $n/m$ subproblems whose column dimensions
are close to the row dimension $m$. This is done by successive column
basis computations. Note that we can also reduce the column dimension
by using successive order basis computations, and only do a column
basis computation at the very last step. The computational complexity
of using order basis computation to reduce the column dimension would
remain the same, but in practice it maybe more efficient since order
basis computations are simpler.
\end{rem}

\section{Column Reduced Form and Popov Form}

Let us now look how column basis computation leads to efficient deterministic
algorithms for computing column reduced form and Popov form for matrices
of any dimension. Since \citet{SS2011} already provided algorithms
to transform column reduced forms to Popov forms, we just need to
consider the problem of computing column reduced form. In addition,
since \citet{GSSV2012} provided a deterministic algorithm for the
column reduction of a square nonsingular input matrix, we just need
to reduce the problem with general input matrix to the square nonsingular
case. For this problem, we only give the cost in terms of the less
refined matrix degree $d$ instead of the sum of the column degrees
and aim for a cost of $O^{\sim}\left(nm^{\omega-1}d\right)$. So there
is more room for improvement here.
\begin{thm}
The column reduced form and Popov form of any matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
can be computed deterministically with a cost of $O^{\sim}\left(nm^{\omega-1}d\right).$\end{thm}
\begin{proof}
We may now assume that the input matrix $\mathbf{F}$ has full column
rank, which can be done by a direct application of the column basis
computation. It only remains to consider the case that the row dimension
$m$ of $\mathbf{F}$ is higher than its column dimension $n$. Using
the transposed version of \prettyref{lem:matrixGCD}, we can factor
$\mathbf{F}$ as $\mathbf{F}=\mathbf{G}\mathbf{T}$, where $\mathbf{G}$
is column reduced and $\mathbf{T}$ is a square nonsingular row basis
of $\mathbf{F}$. Let $\vec{t}=-\cdeg_{\left[-d,\dots,-d\right]}\mathbf{G}$,
or equivalently, $\vec{t}=d-\cdeg\mathbf{G}$, then from \prettyref{lem:colBasisdegreeBoundByRdegOfRightFactor}
we have $\cdeg_{-\vec{t}}\mathbf{T}\le0$, and from \prettyref{lem:colBasisDegreeBoundByInputDegrees}
we know that $\vec{t}\le d$. Now using \prettyref{lem:predictableColumnReducedness},
a $-\vec{t}$-column reduced form $\mathbf{T}'$ of $\mathbf{T}$
makes $\mathbf{G}\mathbf{T}'$ a column reduced form of $\mathbf{F}$.
To compute a $-\vec{t}$-column reduced form $\mathbf{T}'$ of $\mathbf{T}$,
we can just compute a column reduced form of $x^{d-\vec{t}}\mathbf{T}$,
which is a square nonsingular matrix of degree $d$.\end{proof}

