
\chapter{\label{chap:Matrix-GCD}Column Basis}

Column bases are fundamental constructions in polynomial matrix algebra.
As an example, when the row dimension is one (i.e. $m=1$), then finding
a column basis coincides with finding a greatest common divisor (GCD)
of all the polynomials in the matrix. Similarly, the nonzero columns
of column reduced forms, Popov normal forms, and Hermite normal forms
are all column bases satisfying additional degree constraints. A column
reduced form gives a special column basis whose column degrees are
the smallest possible, while Popov and Hermite forms are special column
reduced or shifted column reduced forms satisfying additional conditions
that make them unique. Efficient column basis computation immediately
leads to fast computation for such core procedures as determining
matrix GCDs \citet{BL2000}, column reduced forms \citet{BVP:1988}
and Popov forms \citet{villard96} of $\mathbf{F}$ with any dimension
and rank. Column basis computation %in this paper 
also provides a deterministic alternative to randomized lattice compression
\citet{li:2006,storjohann-villard:2005}.

While column bases are produced by column reduced, Popov and Hermite
forms and considerable work has been done on computing such forms,
for example \citet{bcl:2006,beelen:1988,Giorgi2003,GSSV2012,sarkar2011,SS2011}.
However most of these existing algorithms require that the input matrices
be square nonsingular and so start with existing column bases. It
is however pointed out in \citet{sarkar2011,SS2011} that randomization
can be used to relax the square nonsingular requirement.

In this chapter, we consider the problem of computing a column basis
of an input matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
with $n\ge m$ and column degrees $\vec{s}$. we give a fast, deterministic
algorithm for the computation of a column basis for $\mathbf{F}$
having complexity $O^{\sim}\left(nm^{\omega-1}s\right)$ field operations
in $\mathbb{K}$ with $s$ being the average average column degree
of $\mathbf{F}$. To compute a column basis, we know from \prettyref{cor:unimodularlyReduceToColumnBasis}
that any matrix polynomial $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
can be unimodularly transformed to a column basis by repeatedly working
with the leading column coefficient matrices. However this method
of computing a column basis can be expensive. Indeed one needs to
work with up to $\sum\vec{s}$ such coefficient matrices, which could
involve up to $\sum\vec{s}$ polynomial matrix multiplications.
Before discussing the efficient computation of column basis, it is
useful to look at following relationship between column basis, kernel
basis, and unimodular matrix.
\begin{lem}
\label{lem:unimodular_kernel_columnBasis}Given $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$.
If $\mathbf{U}$ is a unimodular matrix such that $\mathbf{F}\mathbf{U}=\left[0,\mathbf{T}\right]$
gives a full column rank $\mathbf{T}$, then $\mathbf{U}$ can be
separated into two submatrices $\mathbf{U}=\left[\mathbf{U}_{L},\mathbf{U}_{R}\right]$,
where $\mathbf{U}_{L}$ is a kernel basis of $\mathbf{F}$ and $\mathbf{F}\mathbf{U}_{R}=\mathbf{T}$
is a column basis of $\mathbf{F}$. In addition, the kernel basis
$\mathbf{U}_{L}$ can be replaced by any other kernel basis $\mathbf{N}$
of $\mathbf{F}$ and still gives a unimodular matrix $\left[\mathbf{N},\mathbf{U}_{R}\right]$,
which can also be used to unimodularly transform $\mathbf{F}$ to
$\left[0,\mathbf{T}\right]$. \end{lem}
\begin{proof}
Note that $\mathbf{T}$ is a column basis of $\mathbf{F}$ by \prettyref{cor:unimodularlyReduceToColumnBasis}.%
\begin{comment}
Note that $\mathbf{T}$ is a column basis of $\mathbf{F}$ by \prettyref{cor:unimodularlyReduceToColumnBasis}.
\end{comment}
{} It remains to show that $\mathbf{U}_{L}$ is a kernel basis of $\mathbf{F}$.
Since $\mathbf{F}\mathbf{U}_{L}=0$, $\mathbf{U}_{L}$ is generated
by any kernel basis $\mathbf{N}$, that is, $\mathbf{U}_{L}=\mathbf{N}\mathbf{C}$
for some polynomial matrix $\mathbf{C}$. Let $r$ be the rank of
$\mathbf{F}$, which is also the column dimension of $\mathbf{T}$
and $\mathbf{U}_{R}$. Then both $\mathbf{N}$ and $\mathbf{U}_{L}$
have column dimension $n-r$. Hence $\mathbf{C}$ is a square $(n-r)\times(n-r)$
matrix. Now the unimodular matrix $\mathbf{U}$ can be factored as
\[
\mathbf{U}=\left[\mathbf{N}\mathbf{C},\mathbf{U}_{R}\right]=\left[\mathbf{N},\mathbf{U}_{R}\right]\begin{bmatrix}\mathbf{C}\\
 & I
\end{bmatrix},
\]
 implying that both factors $\left[\mathbf{N},\mathbf{U}_{R}\right]$
and $\begin{bmatrix}\mathbf{C}\\
 & I
\end{bmatrix}$ are unimodular. Therefore, $\mathbf{C}$ is unimodular and $\mathbf{U}_{L}=\mathbf{N}\mathbf{C}$
is also a kernel basis. Notice that the unimodular matrix $\left[\mathbf{N},\mathbf{U}_{R}\right]$
also transforms $\mathbf{F}$ to $\left[0,\mathbf{T}\right]$.
\end{proof}
\prettyref{lem:unimodular_kernel_columnBasis} gives the following
result for a unimodular matrix and its inverse.
\begin{cor}
\label{cor:unimodular_kernel_columnBasis2}Let $\mathbf{U}=\left[\mathbf{U}_{L},\mathbf{U}_{R}\right]$
be any unimodular matrix with columns separated arbitrarily to $\mathbf{U}_{L}$
and $\mathbf{U}_{R}$. Let its inverse $\mathbf{V}=\begin{bmatrix}\mathbf{V}_{U}\\
\mathbf{V}_{D}
\end{bmatrix}$, where the row dimension of $\mathbf{V}_{U}$ matches the column
dimension of $\mathbf{U}_{L}$. So we have 
\[
\mathbf{V}\mathbf{U}=\begin{bmatrix}\mathbf{V}_{U}\\
\mathbf{V}_{D}
\end{bmatrix}\begin{bmatrix}\mathbf{U}_{L},\mathbf{U}_{R}\end{bmatrix}=\begin{bmatrix}\mathbf{V}_{U}\mathbf{U}_{L} & \mathbf{V}_{U}\mathbf{U}_{R}\\
\mathbf{V}_{D}\mathbf{U}_{L} & \mathbf{V}_{D}\mathbf{U}_{R}
\end{bmatrix}=\begin{bmatrix}I & 0\\
0 & I
\end{bmatrix}.
\]
Then $\mathbf{V}_{U}\mathbf{U}_{L}=I$ is a column basis of $\mathbf{V}_{U}$
and a row basis of $\mathbf{U}_{L}$, while $\mathbf{V}_{D}\mathbf{U}_{R}=I$
is a column basis of $\mathbf{V}_{D}$ and a row basis of of $\mathbf{U}_{R}$.
In addition, \textup{$\mathbf{V}_{D}$ and $\mathbf{U}_{L}$ are kernel
bases of each other, while $\mathbf{V}_{U}$ and $\mathbf{U}_{R}$
are kernel bases of each other.}\end{cor}
\begin{proof}
This follows directly from \prettyref{lem:unimodular_kernel_columnBasis},
by taking $\mathbf{F}$ from \prettyref{lem:unimodular_kernel_columnBasis}
to be $\mathbf{V}_{U}$, $\mathbf{V}_{D}$, $\mathbf{U}_{L}^{T}$,
and $\mathbf{U}_{R}^{T}$ here.
\end{proof}
To compute a column basis of $\mathbf{F}$, we use the following procedure.
We first compute a right  kernel basis $\mathbf{N}$ of $\mathbf{F}$.
Then we compute a left  kernel basis $\mathbf{G}$ of $\mathbf{N}$.
This matrix $\mathbf{G}$ is a right factor of $\mathbf{F}$, that
is, $\mathbf{F}=\mathbf{T}\mathbf{G}$ for some $\mathbf{T}\in\mathbb{K}\left[x\right]^{m\times r}$.
Then we can compute the left factor $\mathbf{T}$, which is in fact
a column basis of $\mathbf{F}$.
\begin{lem}
\label{lem:matrixGCD}Given $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$%
\begin{comment}
 and has full row rank
\end{comment}
. Let $\mathbf{N}\in\mathbb{K}\left[x\right]^{n\times(n-r)}$ be any
right kernel basis of $\mathbf{F}$, and $\mathbf{G}\in\mathbb{K}\left[x\right]^{r\times n}$
be any left kernel basis of $\mathbf{N}$, where $r$ is the rank
of $\mathbf{F}$. Then $\mathbf{F}=\mathbf{T}\mathbf{G}$ for some
$\mathbf{T}\in\mathbb{K}\left[x\right]^{m\times r}$ and $\mathbf{T}$
is a column basis of $\mathbf{F}$.\end{lem}
\begin{proof}
Let the matrix $\mathbf{U}=\begin{bmatrix}\mathbf{U}_{L},\mathbf{U}_{R}\end{bmatrix}$
from \prettyref{cor:unimodular_kernel_columnBasis2} be a unimodular
matrix that transforms $\mathbf{F}$ to a column basis $\mathbf{B}\in\mathbb{K}\left[x\right]^{m\times r}$
of $\mathbf{F}$, where $\mathbf{U}_{L}$ is any right kernel basis
of $\mathbf{F}$. From $\mathbf{F}\mathbf{U}=\left[0,\mathbf{B}\right]$,
we get\textbf{ $\mathbf{F}=\left[0,\mathbf{B}\right]\mathbf{U}^{-1}=\mathbf{B}\left[0,I\right]\mathbf{V}=\mathbf{B}\mathbf{V}_{D}$}.
Since $\mathbf{V}_{D}$ is a left kernel basis of\textbf{ $\mathbf{U}_{L}$}
by \prettyref{cor:unimodular_kernel_columnBasis2}, any other left
kernel basis $\mathbf{G}$ of $\mathbf{U}_{L}$ is unimodularly equivalent
to $\mathbf{V}_{D}$, that is, $\mathbf{V}_{D}=\mathbf{W}\mathbf{G}$
for some unimodular matrix $\mathbf{W}$. Now $\mathbf{F}=\mathbf{B}\mathbf{W}\mathbf{G}$,
where $\mathbf{BW}=\mathbf{T}$ a column basis of $\mathbf{F}$ since
it is unimodularly equivalent to the column basis $\mathbf{B}$.
\end{proof}


\prettyref{lem:matrixGCD} outlines a procedure for computing a column
basis of $\mathbf{F}$ with three main steps. The first step is to
compute a $\left(\mathbf{F},\vec{s}\right)$-kernel basis $\mathbf{N}$,
which can be efficiently done using \prettyref{alg:minimalNullspaceBasis}.
However, we still need to work on the second step of computing a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{G}^{T}$ and the third step of computing the column
basis $\mathbf{T}$ from $\mathbf{F}$ and $\mathbf{G}$. Note that
while \prettyref{lem:matrixGCD} does not require the bases computed
to be minimal, working with minimal bases keeps the degrees well-managed
and helps to make the computation efficient.
\begin{example}
Let 
\[
\mathbf{F}=\left[\begin{array}{cccc}
x^{2} & x^{2} & x+x^{2} & 1+x^{2}\\
1+x+x^{2} & x^{2} & 1+x^{2} & 1+x^{2}
\end{array}\right]~.
\]
Then the matrix 
\[
\mathbf{N}=\left[\begin{array}{cc}
x & 1\\
1 & x\\
x & 1\\
0 & x
\end{array}\right]
\]
is a right kernel basis of $\mathbf{F}$ and the matrix 
\[
\mathbf{G}=\left[\begin{array}{cccc}
1 & 0 & 1 & 0\\
\noalign{\medskip}x & {x}^{2} & 0 & 1+{x}^{2}
\end{array}\right]
\]
is a left nulllspace basis of $\mathbf{N}$. Finally the matrix 
\[
\mathbf{T}=\left[\begin{array}{cc}
x+x^{2} & 1\\
1+x^{2} & 1
\end{array}\right]
\]
satisfies $\mathbf{F}=\mathbf{T}\mathbf{G}$, and is a column basis
of $\mathbf{F}$. 
\end{example}

\section{\label{sec:computeRightFactor}Computing a Right Factor}

Let us now look at the computation of a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{G}^{T}$. For this problem, \prettyref{alg:minimalNullspaceBasisWithRankProfile}
does not work well directly, since the input matrix $\mathbf{N}^{T}$
has nonuniform row degrees and negative shift. Comparing to the earlier
problem of computing a $\left(\mathbf{F},\vec{s}\right)$-kernel basis
$\mathbf{N}$, it is interesting to note that the old output $\mathbf{N}$
now becomes the new input matrix $\mathbf{N}^{T}$, while the new
output matrix $\mathbf{G}$ has size bounded by $\mathbf{F}$. In
other words, the new input has degrees that matches the old output,
while the new output has degrees bounded by the old input. It is
therefore reasonable to expect that the new problem can be computed
efficiently. However, we need to find some way to work with the more
complicated input degree structure. On the other hand, the simpler
output degree structure makes it easier to apply order basis computation
to compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis. 

To see how order basis computations can be applied here, let us first
extend \prettyref{lem:orderBasisContainsNullspaceBasis}, which provides
a relationship between order bases and kernel bases, to accommodate
our situation here.
\begin{lem}
\label{lem:orderbasisContainsNullspacebasisGeneralized}Given a matrix
$\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$ and a degree
shift $\vec{u}$ with $\rdeg_{\vec{u}}\mathbf{A}\le\vec{v}$, or equivalently,
$\cdeg_{-\vec{v}}\mathbf{A}\le-\vec{u}$. Let $\mathbf{P}=\left[\mathbf{P}_{1},\mathbf{P}_{2}\right]$
be any $\left(\mathbf{A},\vec{v}+1,-\vec{u}\right)$-basis and $\mathbf{Q}=\left[\mathbf{Q}_{1},\mathbf{Q}_{2}\right]$
be any $(\mathbf{A},-\vec{u})$-kernel basis, where $\mathbf{P}_{1}$
and $\mathbf{Q}_{1}$ contain all columns from $\mathbf{P}$ and $\mathbf{Q}$,
respectively, whose $-\vec{u}$-column degrees are no more than $0$.
Then $\left[\mathbf{P}_{1},\mathbf{Q}_{2}\right]$ is an $(\mathbf{A},-\vec{u})$-kernel
basis, and $\left[\mathbf{Q}_{1},\mathbf{P}_{2}\right]$ is an $\left(\mathbf{A},\vec{v}+1,-\vec{u}\right)$-basis.\end{lem}
\begin{proof}
We know $\cdeg_{-\vec{v}}\mathbf{A}\mathbf{P}_{1}\le\cdeg_{-\vec{u}}\mathbf{P}_{1}\le0$,
or equivalently, $\rdeg\mathbf{A}\mathbf{P}_{1}\le\vec{v}$, but it
has order greater than $\vec{v}$, hence $\mathbf{A}\mathbf{P}_{1}=0$.
The result then follows the same reasoning as in the proof of \prettyref{lem:orderBasisContainsNullspaceBasis}.%
\begin{comment}
We know $\cdeg\mathbf{P}_{1}^{T}\le\vec{u}$ from $\cdeg_{-\vec{u}}\mathbf{P}_{1}\le0$,
hence $\cdeg\mathbf{P}_{1}^{T}\mathbf{A}^{T}\le\cdeg_{\vec{u}}\mathbf{A}^{T}$
by \prettyref{lem:boundOnDegreesOfFA}. Now for each row $\mathbf{a}_{i}$
in $\mathbf{A}$ and its $\vec{u}$-column degree $v_{i}$, we have
\[
\rdeg\mathbf{a}_{i}\mathbf{P}_{1}=\cdeg\mathbf{P}_{1}^{T}\mathbf{a}_{i}^{T}\le\cdeg_{\vec{u}}\mathbf{a}_{i}^{T}=v_{i},
\]
 and $\mathbf{a}_{i}\mathbf{P}_{1}\equiv0\mod x^{v_{i}+1}$, hence
$\mathbf{A}\mathbf{P}_{1}=0$. The result then follows the same reasoning
as in the proof of \prettyref{lem:orderBasisContainsNullspaceBasis}.
\end{comment}

\end{proof}
Now with the help of \prettyref{lem:orderbasisContainsNullspacebasisGeneralized},
let us get back to our problem of computing a $(\mathbf{F},\vec{s})$-kernel
basis.  In fact, we just need to use a special case of \prettyref{lem:orderbasisContainsNullspacebasisGeneralized},
where all the elements of the kernel basis have shifted degrees bounded
by $0$, making the partial kernel basis a complete kernel basis%
\begin{comment}
, which follows from our requirement of using a shift $\vec{s}\ge\cdeg\mathbf{F}$
\end{comment}
.
\begin{lem}
\label{lem:nullspaceBasisInOrderBasis}Let $\mathbf{N}$ be a $(\mathbf{F},\vec{s})$-kernel
basis with $\cdeg_{\vec{s}}\mathbf{N}=\vec{b}$. Let $\mathbf{P}=\left[\mathbf{P}_{1},\mathbf{P}_{2}\right]$
be a $\left(\mathbf{N}^{T},\vec{b}+1,-\vec{s}\right)$-basis, where
$\mathbf{P}_{1}$ consists of all columns $\mathbf{p}$ with $\cdeg_{-\vec{s}}\mathbf{p}\le0$.
\begin{comment}
of $\mathbf{P}$ satisfying $\mathbf{N}^{T}\mathbf{p}=0$. 
\end{comment}
Then $\mathbf{P}_{1}$ is a $(\mathbf{N}^{T},-\vec{s})$-kernel basis. \end{lem}
\begin{proof}
Let the rank of $\mathbf{F}$ be $r$, which is also the column dimension
of any $(\mathbf{N}^{T},-\vec{s})$-kernel basis $\mathbf{G}^{T}$.
Since both $\mathbf{F}$ and $\mathbf{G}$ are in the left kernel
of $\mathbf{N}$, we know $\mathbf{F}$ is generated by $\mathbf{G}$,
and the $-\vec{s}$-row degrees of $\mathbf{G}$ are bounded by the
corresponding $r$ largest $-\vec{s}$-row degrees of $\mathbf{F}$,
which are in turn bounded by $0$ since $\cdeg\mathbf{F}\le\vec{s}$.
Therefore, any $(\mathbf{N}^{T},-\vec{s})$-kernel basis $\mathbf{G}^{T}$
satisfies $\cdeg_{-\vec{s}}\mathbf{G}^{T}\le0$. The result now follows
from \prettyref{lem:orderbasisContainsNullspacebasisGeneralized}.
\end{proof}
We can use \prettyref{thm:continueComputingNullspaceBasisByRows}
to compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis
by rows. If we separate $\mathbf{N}$ to $\mathbf{N}=\left[\mathbf{N}_{1},\mathbf{N}_{2}\right]$
with $\vec{s}$-column degrees $\vec{b}_{1}$, $\vec{b}_{2}$ respectively,
and first compute a $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{Q}_{1}$ with $-\vec{s}$-column degrees $-\vec{s}_{2}$,
and then compute a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis $\mathbf{Q}_{2}$, then $\mathbf{Q}_{1}\mathbf{Q}_{2}$ is a
$\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis. To compute kernel
bases $\mathbf{Q}_{1}$ and $\mathbf{Q}_{2}$, we can use order basis
computation. However, we need to make sure that the order bases we
compute do contain these kernel bases.
\begin{lem}
\label{lem:nullspaceBasisOfSubsetOfRowsContainedInOrderBasis}Let
$\mathbf{N}$ be partitioned as $\mathbf{N}=\left[\mathbf{N}_{1},\mathbf{N}_{2}\right]$,
with $\vec{s}$-column degrees $\vec{b}_{1}$, $\vec{b}_{2}$ respectively.
Then a $\left(\mathbf{N}_{1}^{T},\vec{b}_{1}+1,-\vec{s}\right)$-basis
contains a $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel basis
whose $-\vec{s}$-column degrees are bounded by 0. Let $\mathbf{Q}_{1}$
be this kernel basis, and $-\vec{s}_{2}=\cdeg_{-\vec{s}}\mathbf{Q}_{1}$.
Then a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},\vec{b}_{2}+1,-\vec{s}_{2}\right)$-basis
contains a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis $\mathbf{Q}_{2}$ whose $-\vec{s}$-column degrees are bounded
by 0. The product $\mathbf{Q}_{1}\mathbf{Q}_{2}$ is then a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis.\end{lem}
\begin{proof}
To see that a $\left(\mathbf{N}_{1}^{T},\vec{b}_{1}+1,-\vec{s}\right)$-basis
contains a $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel basis
whose $-\vec{s}$-column degrees are bounded by 0, we just need to
show that $\cdeg_{-\vec{s}}\mathbf{\bar{Q}}_{1}\le0$ for any $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{\bar{Q}}_{1}$ and then apply \prettyref{lem:orderbasisContainsNullspacebasisGeneralized}.
Note that there exists a polynomial matrix $\bar{\mathbf{Q}}_{2}$
such that $\mathbf{\bar{Q}}_{1}\mathbf{\bar{Q}}_{2}=\bar{\mathbf{G}}$
for any $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis $\bar{\mathbf{G}}$,
as $\bar{\mathbf{G}}$ satisfies $\mathbf{N}_{1}^{T}\bar{\mathbf{G}}=0$
and is therefore generated by the $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis $\bar{\mathbf{Q}}_{1}$. If $\cdeg_{-\vec{s}}\mathbf{\bar{Q}}_{1}\nleq0$,
then \prettyref{lem:predictableDegree} forces $\cdeg_{-\vec{s}}\left(\bar{\mathbf{Q}}_{1}\bar{\mathbf{Q}}_{2}\right)=\cdeg_{-\vec{s}}\bar{\mathbf{G}}\nleq0$,
a contradiction since we know from the proof of \prettyref{lem:nullspaceBasisInOrderBasis}
that $\cdeg_{-\vec{s}}\mathbf{G}^{T}\le0$. 

As before, to see that a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},\vec{b}_{2}+1,-\vec{s}_{2}\right)$-basis
contains a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis whose $-\vec{s}$-column degrees are no more than 0, we can
just show $\cdeg_{-\vec{s}_{2}}\hat{\mathbf{Q}}_{2}\le0$ for any
$\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis $\hat{\mathbf{Q}}_{2}$ and then apply \prettyref{lem:orderbasisContainsNullspacebasisGeneralized}.
Since $\cdeg_{\vec{s}}\mathbf{N}_{2}=\vec{b}_{2}$, we have $\rdeg_{-\vec{b}_{2}}\mathbf{N}_{2}\le-\vec{s}$
or equivalently, $\cdeg_{-\vec{b}_{2}}\mathbf{N}_{2}^{T}\le-\vec{s}.$
Then combining this with $\cdeg_{-\vec{s}}\mathbf{Q}_{1}=-\vec{s}_{2}$
we get $\cdeg_{-\vec{b}_{2}}\mathbf{N}_{2}^{T}\mathbf{Q}_{1}\le-\vec{s}_{2}$
using \prettyref{lem:predictableDegree}. Let $\hat{\mathbf{G}}=\mathbf{Q}_{1}\hat{\mathbf{Q}}_{2}$,
which is now a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis
by \prettyref{thm:continueComputingNullspaceBasisByRows}. Note that
$\cdeg_{-\vec{s}_{2}}\hat{\mathbf{Q}}_{2}=\cdeg_{-\vec{s}}\mathbf{Q}_{1}\hat{\mathbf{Q}}_{2}=\cdeg_{-\vec{s}}\hat{\mathbf{G}}\le0$. 
\end{proof}
Now that we can correctly compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis by rows with the help of order basis computation using \prettyref{lem:nullspaceBasisOfSubsetOfRowsContainedInOrderBasis},
we need to look at how to do it efficiently. One major difficulty
is that the order $\vec{b}+1$, or equivalently, the $\vec{s}$-row
degrees of $\mathbf{N}_{1}^{T}$ are nonuniform and can have degree
as large as $\sum\vec{s}$. To overcome this, we separate the rows
of $\mathbf{N}^{T}$ into blocks according to their $\vec{s}$-row
degrees, and then work with these blocks one by one successively using
\prettyref{thm:continueComputingNullspaceBasisByRows}. 

\input{algorithmNullspaceBasisReverse.tex}

Let $k$ be the column dimension of $\mathbf{N}$ and $\xi$ be an
upper bound of $\sum\vec{s}$. Since $\sum\cdeg_{\vec{s}}\mathbf{N}=\sum\vec{b}\le\sum\vec{s}\le\xi$,
at most $k/c$ columns of $\mathbf{N}$ have $\vec{s}$-column degrees
greater than or equal to $c\xi/k$ for any $c\ge1$. We assume without
loss of generality that the rows of $\mathbf{N}^{T}$ are arranged
in decreasing $\vec{s}$-row degrees. We divide $\mathbf{N}^{T}$
into $\log k$ row blocks according to the $\vec{s}$-row degrees
of its rows, or equivalently, divide $\mathbf{N}$ to blocks of columns
according to the $\vec{s}$-column degrees. Let 
\[
\mathbf{N}=\left[\mathbf{N}_{1},\mathbf{N}_{2},\cdots,\mathbf{N}_{\log k-1},\mathbf{N}_{\log k}\right]
\]
with $\mathbf{N}_{\log k},\mathbf{N}_{\log k-1},\dots,\mathbf{N}_{2},\mathbf{N}_{1}$
having $\vec{s}$-column degrees in the range $\left[0,2\xi/k\right]$,
$(2\xi/k,4\xi/k],$ $(4\xi/k,8\xi/k],\ ...,$ $(\xi/4,\xi/2],$ $(\xi/2,\xi].$
Let 
\[
\vec{\sigma}_{i}=\left[\xi/2^{i-1}+1,\dots,\xi/2^{i-1}+1\right]
\]
 with the same dimension as the row dimension of $\mathbf{N}_{i}$.
Let 
\[
\vec{\sigma}=\left[\vec{\sigma}_{\log k},\vec{\sigma}_{\log k-1},\dots,\vec{\sigma}_{1}\right]
\]
 be the order in the order basis computation.

To further simply our task, we also make the order of our problem
in each block uniform. Rather than of using $\mathbf{N}^{T}$ as the
input matrix, we use 
\begin{eqnarray*}
\hat{\mathbf{N}} & =\begin{bmatrix}\hat{\mathbf{N}}_{1}\\
\vdots\\
\hat{\mathbf{N}}_{\log k}
\end{bmatrix}= & x^{\vec{\sigma}-\vec{b}-1}\begin{bmatrix}\mathbf{N}_{1}^{T}\\
\vdots\\
\mathbf{N}_{\log k}^{T}
\end{bmatrix}=x^{\vec{\sigma}-\vec{b}-1}\mathbf{N}^{T}
\end{eqnarray*}
 instead, so that a $\left(\hat{\mathbf{N}},\vec{\sigma},-\vec{s}\right)$-basis
is a $\left(\mathbf{N}^{T},\vec{b}+1,-\vec{s}\right)$-basis.

We are now ready to compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis, which is done by a series of order basis computations that
computes a series of kernel bases as follows.

Let $\vec{s}_{1}=\vec{s}$. First we compute an $\left(\hat{\mathbf{N}}_{1},\vec{\sigma}_{1},-\vec{s}_{1}\right)$-basis
$\mathbf{P}_{1}=\left[\mathbf{G}_{1},\mathbf{Q}_{1}\right]$, where
$\mathbf{G}_{1}$ is a $\left(\hat{\mathbf{N}}_{1},-\vec{s}_{1}\right)$-kernel
basis%
\begin{comment}
 with $\cdeg_{-\vec{s}_{1}}\mathbf{N}_{1}\le0$
\end{comment}
.

Let $\tilde{\mathbf{G}}_{1}=\mathbf{G}_{1}$. Let $\vec{s}_{2}=-\cdeg_{-\vec{s}}\mathbf{G}_{1}$.
\begin{comment}
Note that $-\vec{s}_{1}\le-[\vec{s}_{2},\vec{t}_{2}]\le\left[0,\dots,0,1,\dots1\right]$
component-wise, since $\mathbf{P}_{1}$ has lower order than any $\left(\mathbf{M}^{T},\vec{b}+\left[1,\dots,1\right],-\vec{s}\right)$-basis
$\mathbf{P}$ hence generates $\mathbf{P}$. Therefore, $\cdeg_{-\vec{s}}\mathbf{P}_{1}\le\cdeg_{-\vec{s}}\mathbf{P}\le\left[0,\dots,0,1,\dots1\right]$. 
\end{comment}
{} We then compute an $\left(\hat{\mathbf{N}}_{2}\tilde{\mathbf{G}}_{1},\vec{\sigma}_{2},-\vec{s}_{2}\right)$-basis
$\mathbf{P}_{2}=\left[\mathbf{G}_{2},\mathbf{Q}_{2}\right]$ with
$\vec{s}_{3}=-\cdeg_{-\vec{s}_{2}}\mathbf{G}_{2}$. Let $\tilde{\mathbf{G}}_{2}=\tilde{\mathbf{G}}_{1}\mathbf{G}_{2}$.
 %
\begin{comment}
Let $\mathbf{R}_{1}=\left[\mathbf{N}_{1}\mathbf{Q}_{2},\mathbf{Q}_{1}\right]$
and $\mathbf{R}_{1}^{r}=\revCol\left(\mathbf{R}_{1},-\vec{s},\cdeg_{-\vec{s}}\mathbf{R}_{1}\right)$.
Then from \prettyref{lem:unimodularComputationByRows} we know $\left[\mathbf{F}^{T},\mathbf{R}_{1}^{r}\right]$
is a unimodular matrix.
\end{comment}


Continuing this process, at step $i$ we compute an $\left(\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1},\vec{\sigma}_{i},-\vec{s}_{i}\right)$-basis
$\mathbf{P}_{i}=\left[\mathbf{G}_{i},\mathbf{Q}_{i}\right]$. Let
$\tilde{\mathbf{G}}_{i}=\prod_{j=1}^{i}\mathbf{G}_{i}=\tilde{\mathbf{G}}_{i-1}\mathbf{G}_{i}$.
Note that $\tilde{\mathbf{G}}_{\log k}$ is a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis. 

This process of computing a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis gives \prettyref{alg:minimalNullspaceBasisReverse}.

Now let us check the cost of this algorithm. The cost is dominated
by the order basis computation and the multiplications $\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1}$
and $\tilde{\mathbf{G}}_{i-1}\mathbf{G}_{i}$. Let $s=\xi/n$.
\begin{lem}
An $\left(\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1},\vec{\sigma}_{i},-\vec{s}_{i}\right)$-basis
can be computed with a cost of $O^{\sim}\left(n^{\omega}s\right)$.\end{lem}
\begin{proof}
Note that $\mathbf{N}_{i}$ has less than $2^{i}$ columns. Otherwise,
\[
\sum\cdeg_{\vec{s}}\mathbf{N}_{i}>2^{i}\xi/2^{i}=\xi,
\]
contradicting with 
\[
\sum\cdeg_{\vec{s}}\mathbf{N}=\sum\vec{b}\le\sum\vec{s}\le\xi.
\]
It follows that $\hat{\mathbf{N}}_{i}$, and therefore $\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1}$,
also have less than $2^{i}$ rows. We also have $\vec{\sigma}_{i}=\left[\xi/2^{i-1}+1,\dots,\xi/2^{i-1}+1\right]$
with entries in $\Theta\left(\xi/2^{i}\right)$. Therefore, \prettyref{alg:umab}
can be used with a cost of $O^{\sim}\left(n^{\omega}s\right)$ by
\prettyref{thm:unbalancedOrderBasisCost}.\end{proof}
\begin{lem}
The multiplications $\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1}$
can be done with a cost of $O^{\sim}\left(n^{\omega}s\right)$.\end{lem}
\begin{proof}
The dimension of $\hat{\mathbf{N}}_{i}$ is bounded by $2^{i-1}\times n$
and $\sum\rdeg_{\vec{s}}\hat{\mathbf{N}}_{i}\le2^{i-1}\cdot\xi/2^{i-1}=\xi$.
We also have $\cdeg_{-\vec{s}}\tilde{\mathbf{G}}_{i-1}\le0$, or equivalently,
$\rdeg\tilde{\mathbf{G}}_{i-1}\le\vec{s}$. We can now use \prettyref{thm:multiplyUnbalancedMatrices}
to multiply $\tilde{\mathbf{G}}_{i-1}^{T}$ and $\hat{\mathbf{N}}_{i}^{T}$
with a cost of $O^{\sim}\left(n^{\omega}s\right)$.\end{proof}
\begin{lem}
The multiplication $\tilde{\mathbf{G}}_{i-1}\mathbf{G}_{i}$ can be
done with a cost of $O^{\sim}\left(n^{\omega}s\right)$.\end{lem}
\begin{proof}
We know $\cdeg_{-\vec{s}}\tilde{\mathbf{G}}_{i-1}=-\vec{s}_{i}$,
and $\cdeg_{-\vec{s}_{i}}\mathbf{G}_{i}=-\vec{s}_{i+1}\le0.$ In other
words, $\rdeg\mathbf{G}_{i}\le\vec{s}_{i}$, and $\rdeg_{\vec{s}_{i}}\tilde{\mathbf{G}}_{i-1}\le\vec{s}$,
hence we can again use \prettyref{thm:multiplyUnbalancedMatrices}
to multiply $\mathbf{G}_{i}^{T}$ and $\tilde{\mathbf{G}}_{i-1}^{T}$
with a cost of $O^{\sim}\left(n^{\omega}s\right)$.\end{proof}
\begin{lem}
\label{lem:costKernelBasisReverse}Given an input matrix $\mathbf{M}\in\mathbb{K}\left[x\right]^{k\times n}$,
a shift $\vec{s}\in\mathbb{Z}^{n}$, and an upper bound $\xi\in\mathbb{Z}$
such that 
\begin{itemize}
\item $\sum\rdeg_{\vec{s}}\mathbf{M}\le\xi$,
\item $\sum\vec{s}\le\xi$,
\item and any $\left(\mathbf{M},-\vec{s}\right)$-kernel basis having row
degrees bounded by $\vec{s}$, or equivalently, having $-\vec{s}$-column
degrees bounded by 0.
\end{itemize}
Then \prettyref{alg:minimalNullspaceBasisReverse} costs $O^{\sim}\left(n^{\omega}s\right)$
field operations to compute a $\left(\mathbf{M},-\vec{s}\right)$-kernel
basis.

Note that $\xi$ can be simply set to $\sum\vec{s}$.\end{lem}
\begin{thm}
A right factor $\mathbf{G}$ satisfying $\mathbf{F}=\mathbf{TG}$
for a column basis $\mathbf{T}$ can be computed with a cost of $O^{\sim}\left(n^{\omega}s\right)$. 
\end{thm}

\section{Computing a Column Basis}

With a right factor $\mathbf{G}$ of $\mathbf{F}$ computed, we are
now ready to compute a column basis $\mathbf{T}$ using the equation
$\mathbf{F}=\mathbf{T}\mathbf{G}$. To do so efficiently, the degree
of $\mathbf{T}$ cannot be too big, which is indeed the case as shown
by the following lemmas.
\begin{lem}
\label{lem:colBasisdegreeBoundByRdegOfRightFactor}The column degrees
of $\mathbf{T}$ are bounded by the corresponding entries of $\vec{t}=-\rdeg_{-\vec{s}}\mathbf{G}$.\end{lem}
\begin{proof}
Since $\mathbf{G}$ is $-\vec{s}$-row reduced, and $\rdeg_{-\vec{s}}\mathbf{F}\le0$,
by \prettyref{lem:predictableDegree} $\rdeg_{-\vec{t}}\mathbf{T}\le0$,
or equivalently, $\mathbf{T}$ has column degrees bounded by $\vec{t}$.\end{proof}
\begin{lem}
\label{lem:colBasisDegreeBoundByInputDegrees}Let $\vec{t}=-\rdeg_{-\vec{s}}\mathbf{G}$,
a vector with $r$ entries and bounds $\cdeg\mathbf{T}$ from \prettyref{lem:colBasisdegreeBoundByRdegOfRightFactor}.
Let $\vec{s}'$ be the list of the $r$ largest entries of $\vec{s}$.
Then $\vec{t}\le\vec{s}'$.\end{lem}
\begin{proof}
Let $\mathbf{G}'$ be the $-\vec{s}$-row Popov form of $\mathbf{G}$,
and the square matrix $\mathbf{G}"$ consists of only the columns
of $\mathbf{G}'$ that contains pivot entries, and has the rows permuted
so the pivots are in the diagonal. Let $\vec{s}"$ be the list of
the entries in $\vec{s}$ that correspond to the columns of $\mathbf{G}"$
in $\mathbf{G}'$. Note that $\rdeg_{-\vec{s}"}\mathbf{G}"=-\vec{t}"$
is just a permutation of $-\vec{t}$ with the same entries. By the
definition of shifted row degree, $-\vec{t}"$ is the sum of $-\vec{s}"$
and the list of the diagonal pivot degrees, which are nonnegative.
Therefore, $-\vec{t}"\ge-\vec{s}"$. The result then follows as $\vec{t}$
is a permutation of $\vec{t}"$ and $\vec{s}'$ has the largest entries
of $\vec{s}$.
\end{proof}
With the bound on the column degrees of $\mathbf{T}$ determined,
we are now ready to compute $\mathbf{T}$. This is done again using
an order basis computation.
\begin{lem}
Let $\vec{t}'=\left[0,\dots,0,\vec{t}\right]\in\mathbb{Z}^{m+r}$.
Any $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis has the form $\begin{bmatrix}V\\
\bar{\mathbf{T}}
\end{bmatrix}$, where $V\in\mathbb{K}^{m\times m}$ is a unimodular matrix and $\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$
is a column basis of $\mathbf{F}$.\end{lem}
\begin{proof}
Note first that the matrix $\begin{bmatrix}-I\\
\mathbf{T}^{T}
\end{bmatrix}$ is a kernel basis of $\left[\mathbf{F}^{T},\mathbf{G}^{T}\right]$
and is therefore unimodularly equivalent to any other kernel basis.
Hence any other kernel basis has the form $\begin{bmatrix}-I\\
\mathbf{T}^{T}
\end{bmatrix}U=\begin{bmatrix}V\\
\bar{\mathbf{T}}
\end{bmatrix}$, with $U$ and $V=-U$ unimodular. Thus $\mathbf{T}=\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$.
Also note that the $-\vec{t}'$-minimality forces the unimodular matrix
$V$ in any $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis to be degree 0, the same degree as $I$.
\end{proof}
To compute a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis, we can again use order basis computation.
\begin{lem}
Any $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],\vec{s}+1,-\vec{t}'\right)$-basis
contain a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis whose $-\vec{t}'$-row degrees are bounded by 0.\end{lem}
\begin{proof}
As before, \prettyref{lem:orderbasisContainsNullspacebasisGeneralized}
can be used here. We just need to show that a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}'\right)$-kernel
basis has $-\vec{t}'$-row degrees no more than 0, which is true since
$\rdeg_{-\vec{t}'}\begin{bmatrix}I\\
\mathbf{T}^{T}
\end{bmatrix}\le0$.\end{proof}
\begin{example}
Let 
\[
\mathbf{F}=\left[\begin{array}{cccc}
x^{2} & x^{2} & x+x^{2} & 1+x^{2}\\
1+x+x^{2} & x^{2} & 1+x^{2} & 1+x^{2}
\end{array}\right]
\]
with 
\[
\mathbf{G}=\left[\begin{array}{cccc}
1 & 0 & 1 & 0\\
\noalign{\medskip}x & {x}^{2} & 0 & 1+{x}^{2}
\end{array}\right]
\]
being a minimal left kernel basis of a right kernel basis of $\mathbf{F}$.
In order to compute the column basis $\mathbf{T}$ satisfying $\mathbf{F}=\mathbf{T}\mathbf{G}$,
first we can determine $\cdeg\mathbf{T}\le\vec{t}=\left[2,0\right]$
from Lemma \ref{lem:colBasisdegreeBoundByRdegOfRightFactor}. Then
we can compute a $\left[0,0,-\vec{t}\right]$-minimal left kernel
basis of $\begin{bmatrix}\mathbf{F}\\
\mathbf{G}
\end{bmatrix}$. The matrix 
\[
\left[V,\bar{\mathbf{T}}\right]=\left[\begin{array}{cccc}
\noalign{\medskip}1 & 0 & x+{x}^{2} & 1\\
1 & 1 & 1+x & 0
\end{array}\right]
\]
is such a left kernel basis. A column basis can then be computed as
by 
\[
\mathbf{T}=V^{-1}\bar{\mathbf{T}}=\left[\begin{array}{cc}
x+x^{2} & 1\\
1+{x}^{2} & 1
\end{array}\right].
\]

\end{example}
In order to compute a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$
kernel basis efficiently, we notice that we have the same type of
problem as in Section \ref{sec:computeRightFactor} and hence we can
again use Algorithm \ref{alg:minimalNullspaceBasisReverse}. 
\begin{lem}
\label{lem:costOfKernelBasisReversedForLeftFactor}A $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis can be computed using \prettyref{alg:minimalNullspaceBasisReverse}
with a cost of $O^{\sim}\left(n^{\omega}s\right)$, where $s=\xi/n$
is the average column degree of $\mathbf{F}$ as before. \end{lem}
\begin{proof}
Just use the algorithm with input $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],\vec{t}^{*},\xi\right)$.
We can verify the conditions on the input are satisfied.
\begin{itemize}
\item To see that $\sum\rdeg_{\vec{t}^{*}}\left[\mathbf{F}^{T},\mathbf{G}^{T}\right]\le\xi$,
note that from $\vec{t}=-\rdeg_{-\vec{s}}\mathbf{G}$ and \prettyref{lem:columnDegreesRowDegreesSymmetry}
$\cdeg_{\vec{t}}\mathbf{G}\le\vec{s}$, or equivalently, $\rdeg_{\vec{t}}\mathbf{G}^{T}\le\vec{s}$.
Since we also have $\rdeg\mathbf{F}^{T}\le\vec{s}$, it follows that
$\rdeg_{\vec{t}^{*}}\left[\mathbf{F}^{T},\mathbf{G}^{T}\right]\le\vec{s}$. 
\item The second condition $\sum\vec{t}^{*}\le\xi$ follows from \prettyref{lem:colBasisdegreeBoundByRdegOfRightFactor}.
\item The third condition holds since $\begin{bmatrix}-I\\
\mathbf{T}^{T}
\end{bmatrix}$ is a kernel basis with row degrees bounded by $\vec{t}^{*}$.
\end{itemize}
\end{proof}
With a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis $\begin{bmatrix}V\\
\bar{\mathbf{T}}
\end{bmatrix}$ computed, a column basis is then given by $\mathbf{T}~=~\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$.

The complete algorithm for computing a column basis is then given
in Algorithm \ref{alg:colBasis}.

\input{AlgorithmColumnBasis.tex} 
\begin{thm}
\label{thm:columnBasisCost1}A column basis $\mathbf{T}$ of $\mathbf{F}$
can be computed with a cost of $O^{\sim}\left(n^{\omega}s\right)$,
where $s=\xi/n$ is the average column degree of $\mathbf{F}$ as
before. \end{thm}
\begin{proof}
The cost is dominated by the cost of the three kernel basis computations
in the algorithm. The first one is handled by the algorithm from \citet{za2012}
and \prettyref{thm:costGeneral}, while the remaining two are handled
by \prettyref{alg:minimalNullspaceBasisReverse}, \prettyref{lem:costKernelBasisReverse}
and \prettyref{lem:costOfKernelBasisReversedForLeftFactor}.
\end{proof}
\begin{comment}
This can be done by computing a left $\left[0,\dots,0,d,\dots,d\right]$-minimal
kernel basis $\left[\mathbf{T}',V\right]$ of $\left[\mathbf{G}^{T},\mathbf{F}^{T}\right]^{T}$,
where $d$ is the degree of $\mathbf{F},$ $V$ is a unimodular matrix
and $\mathbf{T}'$ is a $m\times m$ matrix. Note that $\left[\mathbf{T}',V\right]$
has $m$ rows since the rank of $\left[\mathbf{G}^{T},\mathbf{F}^{T}\right]^{T}$
is $m$. Also note that since $\left[\mathbf{T},I\right]$ is a left
kernel basis with $\left[0,\dots,0,d,\dots,d\right]$-row degrees
bounded by $d$, the $\left[0,\dots,0,d,\dots,d\right]$-minimal kernel
basis $\left[\mathbf{T}',V\right]$ must also has its $\left[0,\dots,0,d,\dots,d\right]$-row
degrees bounded by $d$, hence the degree of $V$ must be 0. We can
then easily compute $\mathbf{T}=\mathbf{T}'V^{-1}$. 
\end{comment}



\section{\label{sec:successiveColBasisComputation}A Simple Improvement}

When the input matrix $\mathbf{F}$ has column dimension much larger
$n$ than the row dimension $m$, we can separate $\mathbf{F}=\left[\mathbf{F}_{1},\mathbf{F}_{2},\dots,\mathbf{F}_{n/m}\right]$
to $n/m$ blocks, each with dimension $m\times m$, assuming without
loss of generality $n$ is a multiple of $m$, and the columns are
arranged in increasing degrees. We then do a series of column basis
computations. First we compute a column basis $\mathbf{T}_{1}$ of
$\left[\mathbf{F}_{1},\mathbf{F}_{2}\right]$. Then compute a column
basis $\mathbf{T}_{2}$ of $\left[\mathbf{T}_{1},\mathbf{F}_{3}\right]$.
Repeating this process, at step $i$, we compute a column basis $\mathbf{T}_{i}$
of $\left[\mathbf{T}_{i-1},\mathbf{F}_{i+1}\right]$, until $i=n/m-1$,
when a column basis of $\mathbf{F}$ is computed. 
\begin{lem}
At step $i$, computing a column basis $\mathbf{T}_{i}$ of $\left[\mathbf{T}_{i-1},\mathbf{F}_{i+1}\right]$
can be done with a cost of $O^{\sim}\left(m^{\omega}(s_{i}+s_{i+1})/2\right)$
field operations, where $s_{i}=\left(\sum\cdeg\mathbf{F}_{i}\right)/m$.\end{lem}
\begin{proof}
From Lemma \ref{lem:colBasisdegreeBoundByRdegOfRightFactor}, the
column basis $\mathbf{T}_{i-1}$ of $\left[\mathbf{F}_{1},\dots,\mathbf{F}_{i}\right]$
has column degrees bounded by the largest column degrees of $\mathbf{F}_{i}$,
hence $\sum\cdeg\mathbf{T}_{i-1}\le\sum\cdeg\mathbf{F}_{i}$. The
lemma then follows by combining this with the result from Theorem
\ref{thm:columnBasisCost1} that a column basis $\mathbf{T}_{i}$
of $\left[\mathbf{T}_{i-1},\mathbf{F}_{i+1}\right]$ can be computed
with a cost of $O^{\sim}\left(m^{\omega}\bar{s}\right)$, where 
\[
\bar{s}=\left(\sum\cdeg\mathbf{T}_{i-1}+\sum\cdeg\mathbf{F}_{i+1}\right)/2m\le\left(s_{i}+s_{i+1}\right)/2.
\]
 \end{proof}
\begin{thm}
\label{thm:finalCollBasisCost}A column basis of $\mathbf{F}$ can
be computed with a cost of $O^{\sim}\left(m^{\omega}s\right)$, where
$s=\left(\sum\cdeg\mathbf{F}\right)/n$.\end{thm}
\begin{proof}
Summing up the cost of all the column basis computations, 
\begin{eqnarray*}
 &  & \sum_{i=1}^{n/m-1}O^{\sim}\left(m^{\omega}\left(s_{i}+s_{i+1}\right)/2\right)\\
 &  & ~~~~~~~~~~~~~\subset O^{\sim}\left(m^{\omega}\left(\sum_{i=1}^{n/m}s_{i}\right)/(n/m)\right)\\
 &  & ~~~~~~~~~~~~~=O^{\sim}\left(m^{\omega}s\right).
\end{eqnarray*}
\end{proof}
\begin{rem}
In this section, the computational efficiency is improved by reducing
the original problem to about $n/m$ subproblems whose column dimensions
are close to the row dimension $m$. This is done by successive column
basis computations. Note that we can also reduce the column dimension
by using successive order basis computations, and only do a column
basis computation at the very last step. The computational complexity
of using order basis computation to reduce the column dimension would
remain the same, but in practice it maybe more efficient since order
basis computations are simpler. 
\end{rem}

\section{Column Reduced Form and Popov Form}

Let us now look how column basis computation leads to efficient deterministic
algorithms for computing column reduced form and Popov form for matrices
of any dimension. Since \citet{SS2011} already provided algorithms
to transform column reduced forms to Popov forms, we just need to
consider the problem of computing column reduced form. In addition,
since \citet{GSSV2012} provided a deterministic algorithm for the
column reduction of a square nonsingular input matrix, we just need
to reduce the problem with general input matrix to the square nonsingular
case. For this problem, we only give the cost in terms of the less
refined matrix degree $d$ instead of the sum of the column degrees
and aim for a cost of $O^{\sim}\left(nm^{\omega-1}d\right)$. So there
is more room for improvement here.
\begin{thm}
The column reduced form and Popov form of any matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
can be computed deterministically with a cost of $O^{\sim}\left(nm^{\omega-1}d\right).$\end{thm}
\begin{proof}
We may now assume that the input matrix $\mathbf{F}$ has full column
rank, which can be done by a direct application of the column basis
computation. It only remains to consider the case that the row dimension
$m$ of $\mathbf{F}$ is higher than its column dimension $n$. Using
the transposed version of \prettyref{lem:matrixGCD}, we can factor
$\mathbf{F}$ as $\mathbf{F}=\mathbf{G}\mathbf{T}$, where $\mathbf{G}$
is column reduced and $\mathbf{T}$ is a square nonsingular row basis
of $\mathbf{F}$. Let $\vec{t}=-\cdeg_{\left[-d,\dots,-d\right]}\mathbf{G}$,
or equivalently, $\vec{t}=d-\cdeg\mathbf{G}$, then from \prettyref{lem:colBasisdegreeBoundByRdegOfRightFactor}
we have $\cdeg_{-\vec{t}}\mathbf{T}\le0$, and from \prettyref{lem:colBasisDegreeBoundByInputDegrees}
we know that $\vec{t}\le d$. Now using \prettyref{lem:predictableColumnReducedness},
a $-\vec{t}$-column reduced form $\mathbf{T}'$ of $\mathbf{T}$
makes $\mathbf{G}\mathbf{T}'$ a column reduced form of $\mathbf{F}$.
To compute a $-\vec{t}$-column reduced form $\mathbf{T}'$ of $\mathbf{T}$,
we can just compute a column reduced form of $x^{d-\vec{t}}\mathbf{T}$,
which is a square nonsingular matrix of degree $d$.\end{proof}
\begin{example}
To column reduce 
\[
\mathbf{F}=\left[\begin{array}{cc}
{x}^{2} & 1+x+{x}^{2}\\
\noalign{\medskip}{x}^{2} & {x}^{2}\\
\noalign{\medskip}x+{x}^{2} & 1+{x}^{2}\\
\noalign{\medskip}1+{x}^{2} & 1+{x}^{2}
\end{array}\right],
\]
we factor $\mathbf{F}$ as 
\[
\mathbf{F}=\mathbf{G}\mathbf{T}=\left[\begin{array}{cc}
1 & x\\
\noalign{\medskip}0 & {x}^{2}\\
\noalign{\medskip}1 & 0\\
\noalign{\medskip}0 & 1+{x}^{2}
\end{array}\right]\left[\begin{array}{cc}
x+{x}^{2} & 1+{x}^{2}\\
\noalign{\medskip}1 & 1
\end{array}\right]
\]
 as before with a column reduced $\mathbf{G}$. The column degrees
of $\mathbf{G}$ $\cdeg\mathbf{G}=\left[0,2\right]$. So we compute
$\left[0,2\right]$-column reduced form $\mathbf{T}'$ of $\mathbf{T}$
\[
\mathbf{T}'=\left[\begin{array}{cc}
1+x & x+{x}^{2}\\
\noalign{\medskip}0 & 1
\end{array}\right].
\]
 Now 
\[
\mathbf{G}\mathbf{T}'=\left[\begin{array}{cc}
1+x & {x}^{2}\\
\noalign{\medskip}0 & {x}^{2}\\
\noalign{\medskip}1+x & x+{x}^{2}\\
\noalign{\medskip}0 & 1+{x}^{2}
\end{array}\right]
\]
 is a column reduced form of $\mathbf{F}$.\end{example}

