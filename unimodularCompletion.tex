
\chapter{\label{chap:Unimodular-Completion}Unimodular Completion}

Given a matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
with $n>m$ and its column degrees bounded by the entries of a vector
$\vec{s}$, we consider the problem of efficiently computing a matrix
$\mathbf{G}\in\mathbb{K}\left[x\right]^{(n-m)\times n}$ %
\begin{comment}
with $\left(-\vec{s}\right)$-minimal rows 
\end{comment}
such that $\begin{bmatrix}\mathbf{F}\\
\mathbf{G}
\end{bmatrix}$ is unimodular.%
\begin{comment}
let us call the product of the nonzero entries of its smith normal
form the \emph{generalized determinant} of $\mathbf{F}$.

Suppose $\mathbf{F}$ is full-rank with column degrees bounded by
the entries of a shift $\vec{s}\in\mathbb{Z}_{\ge0}^{n}$. We consider
the problem of finding a matrix $\mathbf{G}\in\mathbb{K}\left[x\right]^{(n-m)\times n}$
with $\left(-\vec{s}\right)$-minimal rows such that $\left[\mathbf{F}^{T},\mathbf{G}^{T}\right]^{T}$
has the same determinant as the generalized determinant of $\mathbf{F}$.
In the special case where the generalized determinant of $\mathbf{F}$
is $1$, the problem specializes to the standard unimodular completion
problem, where $\left[\mathbf{F}^{T},\mathbf{G}^{T}\right]^{T}$ is
unimodular. Note that the $\left(-\vec{s}\right)$ shift is chosen
to make the degrees consistent with the degrees of the input matrix
$\mathbf{F}$.
\begin{example}
If $\mathbf{F}=\left[1,0\right]$, $\vec{s}=\left[0,0\right]$. Then
the generalized determinant of $\mathbf{F}$ is $1$. A $\left(-\vec{s}\right)$-minimal
unimodular completion of $\mathbf{F}$ is then $\mathbf{G}=\left[0,1\right]$.
A unimodular completion that is not minimal is $\left[x^{9},1\right]$.
If $\mathbf{F}=\left[x,x^{2}\right]$, then a $\left(-\vec{s}\right)$-minimal
completion that maintains the generalized determinant is again $\left[0,1\right]$.\end{example}
\end{comment}
{} As before, our goal is to do this with a cost of $O^{\sim}\left(n^{\omega-1}\xi\right)$
field operations, where $\xi=\sum\vec{s}$. 

Unimodular completion also provides another way of computing column
bases. To compute a column basis of $\mathbf{A}$, we can compute
a right kernel basis $\mathbf{B}$ of $\mathbf{A}$. If $\mathbf{C}^{T}$
is a unimodular completion of $\mathbf{B}^{T}$, then \prettyref{lem:unimodular_kernel_columnBasis}
tells us that $\mathbf{A}\mathbf{C}$ is a column basis of $\mathbf{A}$.
Note that the kernel basis computation and the multiplication $\mathbf{AC}$
can be done efficiently using the algorithms from earlier chapters,
while an efficient way of computing unimodular completion is provided
in this chapter.

Before discussing the computation of a unimodular completion, we need
to check the existence of unimodular completion for a given matrix.
In fact, a unimodular completion does not exist for some input matrices,
as in the case of $\mathbf{F}=\left[0,x\right]$. So we need to know
what type of input matrices admit a unimodular completion.
\begin{lem}
\label{lem:unimodularCompletionCondition}A unimodular completion
of $\mathbf{F}$ exists if and only if $\mathbf{F}$ has unimodular
column bases. \end{lem}
\begin{proof}
If $\mathbf{F}$ has a non-unimodular column basis $\mathbf{A}$,
then $\diag\left(\left[\mathbf{A},I\right]\right)$ is always a factor
of $\begin{bmatrix}\mathbf{F}\\
\mathbf{B}
\end{bmatrix}$ for any polynomial matrix $\mathbf{B}$, implying that the matrix
$\begin{bmatrix}\mathbf{F}\\
\mathbf{B}
\end{bmatrix}$ is non-unimodular. On the other hand, if $\mathbf{F}$ has a unimodular
column basis, then there exists a unimodular matrix $\mathbf{U}$
such that $\mathbf{F}\mathbf{U}=\left[I_{m},0\right]$, or $\mathbf{F}=\left[I_{m},0\right]\mathbf{U}^{-1}$
after rearranging, that is, $\mathbf{F}$ must be consists of the
top $m$ rows of $\mathbf{U}^{-1}$. The matrix $\mathbf{U}^{-1}$
is therefore a unimodular completion of the matrix $\mathbf{F}$.
\end{proof}
Since a unimodular completion is only possible for input matrices
with unimodular column bases, we assume for simplicity this is the
case with our input matrix $\mathbf{F}$. This also requires $\mathbf{F}$
to be full rank. For other matrices without unimodular column bases,
we will see later that our method computes a matrix completion for
a row basis of $\mathbf{F}$ with its column basis factor removed.

The proof of \prettyref{lem:unimodularCompletionCondition} shows
that a unimodular completion of $\mathbf{F}$ can be obtained from
the unimodular matrix $\mathbf{U}$ that transforms $\mathbf{F}$
to its column bases. However, we may not be able to compute this $\mathbf{U}$
efficiently since its degree might be too large. More specifically,
$\mathbf{U}$ contains a kernel basis of $\mathbf{F}$ that may have
degree $\xi$, while each of the remaining columns of $\mathbf{U}$
may also have degree $\xi$. 

Before discussing the actual matrix completion, let us look at the
operations that reverses the coefficients of a polynomial, the coefficients
of the polynomial entries of a vector, and the coefficients of the
polynomial entries of a polynomial matrix. These operations are needed
in the computation of our matrix completion.


\section{Reversing polynomial coefficients}

First let us look at the operation that reverses the coefficients
of a polynomial.
\begin{defn}
For a polynomial $p=p_{0}+p_{1}x+\dots+p_{u}x^{u}\in\mathbb{K}\left[x\right]$
with degree bounded by $u$, we define the operation 
\[
\rev(p,u)=\left(p(x^{-1})\right)x^{u}=p_{u}+p_{u-1}x+\cdots+p_{1}x^{u-1}+p_{0}x^{u}.
\]

\end{defn}
We now extend this definition to column vectors and row vectors with
shifted degrees.
\begin{defn}
Let $\vec{u}=\left[u_{1},\dots u_{n}\right]\in\mathbb{Z}^{n}$ be
a degree shift, and a column vector $\mathbf{a}\in\mathbb{K}\left[x\right]^{n\times1}$
with $\vec{u}$-column degree bounded by $v$. We define
\[
\colRev(\mathbf{a},\vec{u},v)=x^{-\vec{u}}\left(\mathbf{a}(x^{-1})\right)x^{v}=\begin{bmatrix}\rev(p,v-u_{1})\\
\vdots\\
\rev(p,v-u_{n})
\end{bmatrix}.
\]
Similarly for a row vector $\mathbf{b}\in\mathbb{K}\left[x\right]^{1\times n}$
with $\vec{u}$-row degree bounded by $v$, where $\vec{u}=\left[u_{1},\dots u_{n}\right]\in\mathbb{Z}^{n}$
is a degree shift, we define
\[
\rowRev(\mathbf{b},\vec{u},v)=\colRev(\mathbf{b}^{T},\vec{u},v)^{T}=x^{v}\left(\mathbf{b}(x^{-1})\right)x^{-\vec{u}}.
\]
\end{defn}
\begin{example}
If $\mathbf{f}=\left[10+x,5+x+2x^{2}\right]$, $\vec{u}=\left[-1,-2\right]$,
and $v=0$, then 
\[
\rowRev(\mathbf{f},\vec{u},v)=x^{0}\left[10+x^{-1},5+x^{-1}+2x^{-2}\right]\begin{bmatrix}x\\
 & x^{2}
\end{bmatrix}=\left[10x+1,5x^{2}+x+2\right].
\]

\end{example}
We can extend the reverse operation further to polynomial matrices.
\begin{defn}
Let $\vec{u}=\left[u_{1},\dots u_{n}\right]\in\mathbb{Z}^{n}$ be
a degree shift. Let $\mathbf{A}\in\mathbb{K}\left[x\right]^{n\times k}$
with $\vec{u}$-column degrees bounded component-wise by $\vec{v}=\left[v_{1},\dots,v_{k}\right]$,
we define 
\[
\colRev(\mathbf{A},\vec{u},\vec{v})=x^{-\vec{u}}\left(\mathbf{A}(1/x)\right)x^{\vec{v}}
\]
 Similarly, for $\vec{u}=\left[u_{1},\dots,u_{n}\right]$ and $\mathbf{B}\in\mathbb{K}\left[x\right]^{k\times n}$
with $\vec{u}$-row degrees bounded component-wise by $\vec{v}=\left[v_{1},\dots,v_{k}\right]$,
\[
\rowRev(\mathbf{B},\vec{u},\vec{v})=\colRev(\mathbf{B}^{T},\vec{u},\vec{v})^{T}=x^{\vec{v}}\left(\mathbf{B}(1/x)\right)x^{-\vec{u}}
\]
 Note that we also have $\rowRev(\mathbf{B},\vec{u},\vec{v})=\colRev(\mathbf{B},-\vec{v},-\vec{u})$.
\end{defn}
\begin{comment}
Again, we assume a shift $\vec{s}$ bounds the column degrees of $\mathbf{F}$
component-wise with $\sum\vec{s}=\xi$. First, we compute a $\vec{s}$-minimal
kernel basis $\mathbf{N}$ of $\mathbf{F}$. We then reverse the coefficients
of $\mathbf{N}$ based on its $\vec{s}$-column degrees as follows:
To reverse the coefficients of column $\mathbf{n}$ that has $\vec{s}$-column
degrees bounded by $t$, let
\[
\reverse(\mathbf{n},\vec{s},t)=x^{-\vec{s}}\left(\mathbf{n}(1/x)\right)x^{t}=\begin{bmatrix}x^{-s_{1}}\\
 & \ddots\\
 &  & x^{-s_{n}}
\end{bmatrix}\left(\mathbf{n}(1/x)\right)x^{t}.
\]
 I.e., for the $i$th entry $\mathbf{n}_{i}$ of $\mathbf{n}$, where
$\mathbf{n}_{i}=p_{0}+p_{1}x+\dots+p_{t-s_{i}}x^{t-s_{i}}$, the reversed
$\mathbf{n}$ becomes $\reverse(\mathbf{n},\vec{s},t)=p_{t-s_{i}}+p_{t-s_{i}-1}x+\cdots+p_{1}x^{t-s_{i}-1}+p_{0}x^{t-s_{i}}.$
Each column of $\mathbf{N}$ is reversed in this way to get a new
matrix $\bar{\mathbf{N}}$. That is, for matrix $\mathbf{N}\in\mathbb{K}\left[x\right]^{n\times k}$
with $\vec{s}$-column degrees bounded component-wise by $\vec{t}$,
we define 
\[
\reverse(\mathbf{N},\vec{s},\vec{t})=x^{-\vec{s}}\left(\mathbf{N}(1/x)\right)x^{\vec{t}}=\begin{bmatrix}x^{-s_{1}}\\
 & \ddots\\
 &  & x^{-s_{n}}
\end{bmatrix}\left(\mathbf{N}(1/x)\right)\begin{bmatrix}x^{t_{1}}\\
 & \ddots\\
 &  & x^{t_{k}}
\end{bmatrix}.
\]


We then compute a $\left(\bar{\mathbf{N}}^{T},\sigma,-\vec{s}\right)$-basis
$\bar{\mathbf{P}}'$ with $\sigma$ big enough to contain a complete
kernel basis $\bar{\mathbf{F}}'$ of $\bar{\mathbf{N}}^{T}$. Let
$\mathbf{P}$ and $\mathbf{F'}$ be the matrices $\bar{\mathbf{P}}'$
and $\mathbf{\bar{F}'}$ with coefficients reversed based on their
$(-\vec{s})$-column degrees respectively. Then it is not difficult
to see that $\mathbf{F}'$ is a kernel basis of $\mathbf{N}'$.
\end{comment}


It is useful to note that any degree bound remains the same after
the reverse operations.
\begin{lem}
If $\mathbf{A}\in\mathbb{K}\left[x\right]^{n\times k}$ has $\vec{u}$-column
degrees bounded by the corresponding entries of $\vec{v}$, then $\colRev(\mathbf{A},\vec{u},\vec{v})$
also has $\vec{u}$-column degrees bounded by the corresponding entries
of $\vec{v}$.
\end{lem}
As one would expect, applying two reverse operations gives back the
original input.
\begin{lem}
The following equalities holds:%
\begin{comment}
\begin{eqnarray*}
\rev\left(\rev(p,u),u\right) & = & p\\
\colRev\left(\colRev(\mathbf{a},\vec{u},v),\vec{u},v\right) & = & \mathbf{a}\\
\rowRev\left(\rowRev(\mathbf{b},\vec{u},v),\vec{u},v\right) & = & \mathbf{b}
\end{eqnarray*}
\end{comment}
\begin{eqnarray*}
\colRev\left(\colRev(\mathbf{A},\vec{u},\vec{v}),\vec{u},\vec{v}\right) & = & \mathbf{A}\\
\rowRev\left(\rowRev(\mathbf{B},\vec{u},\vec{v}),\vec{u},\vec{v}\right) & = & \mathbf{B}
\end{eqnarray*}

\end{lem}
Let us look at a degree bound on the product of a row vector and a
column vector, based on their shifted degrees, when opposite shifts
are used.
\begin{lem}
\label{lem:vectorProductBound}If $\mathbf{a}\in\mathbb{K}\left[x\right]^{1\times n}$
and $\mathbf{a}^{T}$ has $\left(-\vec{u}\right)$-column degree bounded
by $\alpha$ (or equivalently, $\mathbf{a}$ has $\left(-\vec{u}\right)$-row
degree bounded by $\alpha$) and $\mathbf{b}\in\mathbb{K}\left[x\right]^{n\times1}$
has $\vec{u}$-column degree bounded by $\beta$, then $\mathbf{a}\mathbf{b}$
has degree bounded by $\alpha+\beta$.\end{lem}
\begin{proof}
Since $\mathbf{a}x^{-\vec{u}}$ has degree bounded by $\alpha$ and
$x^{\vec{u}}\mathbf{b}$ has degree bounded by $\beta$, $\mathbf{a}x^{-\vec{u}}x^{\vec{u}}\mathbf{b}=\mathbf{a}\mathbf{b}$
has degree bounded by $\alpha+\beta$.
\end{proof}
The following lemma shows that the reverse operation and the multiplication
are commutative when we use the opposite shifts.
\begin{lem}
\label{lem:reverseProduct}If $\mathbf{a}\in\mathbb{K}\left[x\right]^{1\times n}$
has $\left(-\vec{u}\right)$-row degree bounded by $\alpha$ and $\mathbf{b}\in\mathbb{K}\left[x\right]^{n\times1}$
has $\vec{u}$-column degree bounded by $\beta$, then 
\[
\rowRev(\mathbf{a},-\vec{u},\alpha)\cdot\colRev(\mathbf{b},\vec{u},\beta)=\rev(\mathbf{a}\mathbf{b},\alpha+\beta).
\]
\end{lem}
\begin{proof}
\ 
\begin{eqnarray*}
 &  & \rowRev(\mathbf{a},-\vec{u},\alpha)\cdot\colRev(\mathbf{b},\vec{u},\beta)\\
 & = & x^{\alpha}\left(\mathbf{a}(1/x)\right)x^{\vec{u}}x^{-\vec{u}}\left(\mathbf{b}(1/x)\right)x^{\beta}\\
 & = & \left(\mathbf{a}(1/x)\right)x^{\vec{u}}x^{-\vec{u}}\left(\mathbf{b}(1/x)\right)x^{\alpha+\beta}\\
 & = & \left(\mathbf{a}(1/x)\right)\left(\mathbf{b}(1/x)\right)x^{\alpha+\beta}\\
 & = & \left(\left(\mathbf{a}\mathbf{b}\right)(1/x)\right)x^{\alpha+\beta}\\
 & = & \rev(\mathbf{a}\mathbf{b},\alpha+\beta)
\end{eqnarray*}
\begin{comment}
\begin{lem}
If $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$ has $\left(-\vec{s}\right)$-row
degree bounded component-wise by $\vec{a}=\left[a_{1},\dots,a_{m}\right]$
and $\mathbf{G}\in\mathbb{K}\left[x\right]^{n\times k}$ has $\vec{s}$-column
degree bounded component-wise by $\vec{b}=[b_{1},\dots,b_{m}]$, then
\[
\reverse(\mathbf{F},-\vec{s},\vec{a})\cdot\reverse(\mathbf{G},\vec{s},\vec{b})=\reverse(\mathbf{F}\mathbf{G},\vec{c}),
\]
 where 
\[
\vec{c}=\begin{bmatrix}a_{1}+b_{1} & a & \cdots & a_{1}+b_{k}\\
\\
\\
\end{bmatrix}\in\mathbb{Z}^{m\times k}
\]
\end{lem}
\begin{proof}
sd$\reverse(\mathbf{n},\vec{s},t)\cdot\reverse(\mathbf{g},\vec{s},b)$\end{proof}
\end{comment}


We also have the following similar result on the reverse operation
and matrix multiplication\end{proof}
\begin{lem}
\label{lem:reverseMatrixProduct}If $\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$
has $\vec{u}$-column degrees bounded by $\vec{v}$, and $\mathbf{B}\in\mathbb{K}\left[x\right]^{n\times k}$
has $\vec{v}$-column degrees bounded by $\vec{w}$, then 
\[
\colRev(\mathbf{A},\vec{u},\vec{v})\colRev(\mathbf{B},\vec{v},\vec{w})=\colRev(\mathbf{A}\mathbf{B},\vec{u},\vec{w})
\]
 has $\vec{u}$-column degrees bounded by $\vec{w}$. \end{lem}
\begin{proof}
\ 
\begin{eqnarray*}
 &  & \colRev(\mathbf{A},\vec{u},\vec{v})\colRev(\mathbf{B},\vec{v},\vec{w})\\
 & = & x^{-\vec{u}}\left(\mathbf{A}(1/x)\right)x^{\vec{v}}x^{-\vec{v}}\left(\mathbf{B}(1/x)\right)x^{\vec{w}}\\
 & = & x^{-\vec{u}}\left(\mathbf{A}\mathbf{B}\right)(1/x)x^{\vec{w}}.
\end{eqnarray*}
\end{proof}
\begin{lem}
\label{lem:reverseMatrixProduct2}If $\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$
has $\vec{u}$-row degrees bounded by $\vec{v}$, and $\mathbf{B}\in\mathbb{K}\left[x\right]^{n\times k}$
has $-\vec{u}$-column degrees bounded by $\vec{w}$, then 
\[
\rowRev(\mathbf{A},\vec{u},\vec{v})\colRev(\mathbf{B},-\vec{u},\vec{w})=\colRev(\mathbf{A}\mathbf{B},-\vec{v},\vec{w}).
\]
.\end{lem}
\begin{proof}
\begin{eqnarray*}
 &  & \rowRev(\mathbf{A},\vec{u},\vec{v})\colRev(\mathbf{B},-\vec{u},\cdeg_{-\vec{u}}\mathbf{B})\\
 & = & x^{\vec{v}}\left(\mathbf{A}(1/x)\right)x^{-\vec{u}}x^{\vec{u}}\left(\mathbf{B}(1/x)\right)x^{\cdeg_{-\vec{u}}\mathbf{B}}\\
 & = & x^{\vec{v}}\left(\mathbf{A}(1/x)\right)\left(\mathbf{B}(1/x)\right)x^{\cdeg_{-\vec{u}}\mathbf{B}}\\
 & = & x^{\vec{v}}\left(\mathbf{A}\mathbf{B}(1/x)\right)x^{\cdeg_{-\vec{u}}\mathbf{B}}.
\end{eqnarray*}

\end{proof}

\section{Unimodular completion}

In this section, we look at how a unimodular completion can be done
using a combination of kernel basis computations, order basis computations,
and reverse operations. First, we have the following natural relationship
between a kernel basis and the reverse operation.
\begin{lem}
\label{lem:reverseNullspaceBasis}Let $\vec{u}\in\mathbb{Z}^{n}$,
$\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$ with $(-\vec{u})$-row
degrees bounded component-wise by $\vec{a}$, and $\mathbf{A}^{r}=\rowRev\left(\mathbf{A},-\vec{u},\vec{a}\right)$.
Then a matrix $\mathbf{N}\in\mathbb{K}\left[x\right]^{n\times k}$
with $\vec{u}$-column degrees $\vec{b}$ is a $(\mathbf{A},\vec{u})$-kernel
basis %
\begin{comment}
$\vec{u}$-minimal kernel basis of $\mathbf{A}$ 
\end{comment}
if and only if $\mathbf{N}^{r}=\colRev\left(\mathbf{N},\vec{u},\vec{b}\right)$
is a $(\mathbf{A}^{r},\vec{u})$-kernel basis%
\begin{comment}
$\vec{u}$-minimal kernel basis of $\rowRev\left(\mathbf{A},-\vec{u},\vec{a}\right)$
\end{comment}
.\end{lem}
\begin{proof}
If $\mathbf{N}$ is a kernel basis of $\mathbf{A}$, then we know
from \prettyref{lem:reverseProduct} that 
\[
\rowRev\left(\mathbf{A},-\vec{u},\vec{a}\right)\cdot\colRev\left(\mathbf{N},\vec{u},\vec{b}\right)=0,
\]
so $\colRev\left(\mathbf{N},\vec{u},\vec{b}\right)$ is a kernel basis
of $\rowRev\left(\mathbf{A},-\vec{u},\vec{a}\right)$. Suppose $\colRev\left(\mathbf{N},\vec{u},\vec{b}\right)$
is not $\vec{u}$-minimal and we have another kernel basis $\mathbf{M}$
of $\rowRev\left(\mathbf{A},-\vec{u},\vec{a}\right)$ with $\vec{u}$-column
degrees $\vec{c}$ that has some entry lower than the corresponding
entry in $\vec{b}$. Then $\colRev\left(\mathbf{M},\vec{u},\vec{c}\right)$
is also a kernel of $\mathbf{A}$ with lower $\vec{u}$-column degrees
than $\vec{b}$, contradicting the $\vec{u}$-minimality of $\mathbf{N}$.
\end{proof}


The following lemma shows the unimodular equivalence between any matrix
$\mathbf{A}$ that has a unimodular column basis, and a left kernel
basis of any right kernel basis of $\mathbf{A}$.
\begin{lem}
\label{lem:unimodularEquivalenceNullspaceBasisOfNullspaceBasis}Given
a matrix $\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$ with
unimodular column basis. Let $\mathbf{N}\in\mathbb{K}\left[x\right]^{n\times\left(n-m\right)}$
be a right kernel basis of $\mathbf{A}$. Let $\mathbf{B}$ be a left
kernel basis of $\mathbf{N}$. Then $\mathbf{A}=\mathbf{U}\mathbf{B}$
for a unimodular matrix $\mathbf{U}$.\end{lem}
\begin{proof}
This follows from \prettyref{lem:matrixGCD}, which tells us that
$\mathbf{U}$ is just a column basis of $\mathbf{A}$.
\end{proof}
Now let us look at how an order basis can lead to a unimodular matrix. 
\begin{lem}
\label{lem:reverseOrderBasisToUnimodular}Let $\vec{u}=\left[u_{1},\dots u_{n}\right]\in\mathbb{Z}^{n}$
be a degree shift. Any $\left(\mathbf{A},\sigma,\vec{u}\right)$-basis
$\mathbf{P}$ with $\cdeg_{\vec{u}}\mathbf{P}=\vec{v}=\left[v_{1},\dots,v_{k}\right]$
has $\det\left(\mathbf{P}\right)=cx^{\sum\vec{v}-\sum\vec{u}}$ and
$\det\left(\colRev(\mathbf{P},\vec{u},\vec{v})\right)=c$ for some
constant $c\in\mathbb{K}$. In other words, $\colRev(\mathbf{P},\vec{u},\vec{v})$
is unimodular.\end{lem}
\begin{proof}
To see that $\det\left(\mathbf{P}\right)=cx^{\sum\vec{v}-\sum\vec{u}}$,
note that an identity matrix is an $\left(\mathbf{A},0,\vec{u}\right)$-basis,
which has $\vec{u}$-column degrees $\vec{u}$ and determinant $1$.
Then the $\vec{u}$-column degrees only increases by multiplying some
column of $\mathbf{P}$ by $x$. The second property $\det\left(\colRev(\mathbf{P},\vec{u},\vec{v})\right)=c$
follows from the definition 
\[
\colRev(\mathbf{P},\vec{u},\vec{v})=x^{-\vec{u}}\left(\mathbf{P}(1/x)\right)x^{\vec{v}}.
\]

\end{proof}
\prettyref{lem:reverseOrderBasisToUnimodular} suggests that a unimodular
completion of $\mathbf{F}$ can be computed by embedding $\mathbf{F}$
in a reversed order basis, or equivalently, embedding a reversed $\mathbf{F}$
in an order basis. The next question is therefore how to embed a matrix
in an order basis. Recall that \prettyref{lem:nullspaceBasisInOrderBasis}
shows how kernel bases can be embedded in order bases. Therefore,
if we can make the reversed $\mathbf{F}$ a kernel basis of some matrix
$\mathbf{M}$, then there is an order basis of $\mathbf{M}$ that
contains the reversed $\mathbf{F}$. A natural choice for $\mathbf{M}$
is a kernel basis of the reversed $\mathbf{F}$.  We actually have
two choices here. We can either reverse the coefficients of $\mathbf{F}$,
as we do in \prettyref{lem:unimodularComputation} below, or we can
reverse the coefficients of a kernel basis of $\mathbf{F}$.
\begin{lem}
\label{lem:unimodularComputation}Let $\mathbf{F}^{r}=\rowRev\left(\mathbf{F},-\vec{s},0\right)$
and $\mathbf{M}$ be a $(\mathbf{F}^{r},\vec{s})$-kernel basis with
$\cdeg_{\vec{s}}\mathbf{M}=\vec{b}$. Let $\mathbf{P}=\left[\mathbf{P}_{1},\mathbf{P}_{2}\right]$
be a $\left(\mathbf{M}^{T},\vec{b}+\left[1,\dots,1\right],-\vec{s}\right)$-basis,
where $\mathbf{P}_{1}$ consists of all columns $\mathbf{p}$ with
$\cdeg_{-\vec{s}}\mathbf{p}\le0$. If $\mathbf{P}_{2}^{r}=\colRev\left(\mathbf{P}_{2},-\vec{s},\cdeg_{-\vec{s}}\mathbf{P}_{2}\right)$,
then $\left[\mathbf{F}^{T},\mathbf{P}_{2}^{r}\right]$ is a unimodular
matrix.\end{lem}
\begin{proof}
Let $\mathbf{P}_{1}^{r}=\colRev\left(\mathbf{P}_{1},-\vec{s},\cdeg_{-\vec{s}}\mathbf{P}_{1}\right)$.
We know from \prettyref{lem:reverseOrderBasisToUnimodular} that $\left[\mathbf{P}_{1}^{r},\mathbf{P}_{2}^{r}\right]$
is unimodular. Let $\mathbf{M}^{r}=\colRev\left(\mathbf{M},\vec{s},\vec{b}\right)$.
Then from \prettyref{lem:reverseNullspaceBasis} we know $\mathbf{M}^{r}$
is a $\left(\mathbf{F},\vec{s}\right)$-kernel basis and $\mathbf{P}_{1}^{r}$
is a $\left(\left(\mathbf{M}^{r}\right)^{T},-\vec{s}\right)$-kernel
basis, hence by \prettyref{lem:unimodularEquivalenceNullspaceBasisOfNullspaceBasis}
$\mathbf{F}=\mathbf{U}\left(\mathbf{P}_{1}^{r}\right)^{T}$ for some
unimodular matrix $\mathbf{U}$. Now $\left[\mathbf{F}^{T},\mathbf{P}_{2}^{r}\right]^{T}=\diag\left(\left[\mathbf{U},I\right]\right)\left[\mathbf{P}_{1}^{r},\mathbf{P}_{2}^{r}\right]^{T}$.
\end{proof}
\prettyref{lem:unimodularComputation} provides a way to correctly
compute a unimodular completion of $\mathbf{F}$. To improve the computational
efficiency, we can in fact separate the rows of $\mathbf{M}^{T}$
and just work with one subset of rows at a time. 
\begin{lem}
\label{lem:unimodularComputationByRows}Let $\mathbf{F}^{r}=\rowRev\left(\mathbf{F},-\vec{s},0\right)$.
Let $\mathbf{M}$ be a $(\mathbf{F}^{r},\vec{s})$-kernel basis with
$\cdeg_{\vec{s}}\mathbf{M}=\vec{b}$. Let $\mathbf{M}=\left[\mathbf{M}_{1},\mathbf{M}_{2}\right]$.
Let $\mathbf{P}_{1}=\left[\mathbf{N}_{1},\mathbf{Q}_{1}\right]$ be
a $\left(\mathbf{M}_{1}^{T},\cdeg_{\vec{s}}\mathbf{M}_{1}+\left[1,\dots,1\right],-\vec{s}\right)$-basis,
where $\mathbf{N}_{1}$ consists of all columns $\mathbf{p}$ of $\mathbf{P}_{1}$
with $\cdeg_{-\vec{s}}\mathbf{p}\le0$. Let $\vec{t}=\cdeg_{-\vec{s}}\mathbf{N}_{1}$
and $\mathbf{P}_{2}=\left[\mathbf{N}_{2},\mathbf{Q}_{2}\right]$ be
a $\left(\mathbf{M}_{2}^{T}\mathbf{N}_{1},\cdeg_{\vec{s}}\mathbf{M}_{2}+\left[1,\dots,1\right],\vec{t}\right)$-basis,
where $\mathbf{N}_{2}$ consists of all columns $\mathbf{p}$ of $\mathbf{P}_{2}$
with $\cdeg_{-\vec{t}}\mathbf{p}\le0$. Let $\mathbf{R}=\left[\mathbf{N}_{1}\mathbf{Q}_{2},\mathbf{Q}_{1}\right]$
and $\mathbf{R}^{r}=\colRev\left(\mathbf{R},-\vec{s},\cdeg_{-\vec{s}}\mathbf{R}\right)$.
Then $\left[\mathbf{F}^{T},\mathbf{R}^{r}\right]$ is a unimodular
matrix.\end{lem}
\begin{proof}
We know from \prettyref{lem:reverseOrderBasisToUnimodular} that $\mathbf{P}_{1}^{r}=\colRev\left(\mathbf{P}_{1},-\vec{s},\cdeg_{-\vec{s}}\mathbf{P}_{1}\right)$
and $\mathbf{P}_{2}^{r}=\colRev\left(\mathbf{P}_{1},\vec{t},\cdeg_{\vec{t}}\mathbf{P}_{2}\right)$
are both unimodular. Hence $\mathbf{P}_{1}^{r}\cdot\diag\left(\left[\mathbf{P}_{2}^{r},I\right]\right)=\left[\mathbf{N}_{1}^{r}\mathbf{N}_{2}^{r},\mathbf{N}_{1}^{r}\mathbf{Q}_{2}^{r},\mathbf{Q}_{1}\right]=\left[\mathbf{N}_{1}^{r}\mathbf{N}_{2}^{r},\mathbf{R}^{r}\right]$
is unimodular, where\textbf{ $\mathbf{N}_{1}\mathbf{N}_{2}$ }is a
kernel basis of $\mathbf{M}$. The result follows by the same reasoning
as in \prettyref{lem:unimodularComputation}.
\end{proof}

\section{Efficient Computation}

\prettyref{lem:unimodularComputationByRows} provides a way to correctly
compute a unimodular completion of $\mathbf{F}$. Our next task is
to make sure it can be computed efficiently and analyze its computational
cost. We already know that a $(\mathbf{F}^{r},\vec{s})$-kernel basis
can be computed with a cost of $O^{\sim}\left(n^{\omega-1}\xi\right)$.
Therefore, it only remains to check the cost of the order basis computations.
Note that the non-uniform order makes our problem here a little more
difficult. But on the other hand, the output basis has its $-\vec{s}$-column
degrees bounded by $1$, which is a consequence of the fact $\mathbf{M}$
is a $\vec{s}$-minimal kernel basis, as shown in \prettyref{lem:nullspaceOrderbasisDegree}
below. But we first need a few general lemmas on the degree bounds
of order bases and kernel bases.

First, the following lemma is a simple extension of \prettyref{lem:boundOfSumOfShiftedDegreesOfOrderBasis}
for dealing with nonuniform orders.
\begin{lem}
\label{lem:boundOfSumOfShiftedDegreesOfOrderBasisWithNonuniformOrder}Given
an input matrix $\mathbf{A}\in\mathbb{K}^{m\times n}[x]$, a shift
$\vec{u}\in\mathbb{Z}^{n}$, and an order list $\vec{\sigma}\in\mathbb{Z}^{m}$.
Let $\vec{v}$ be the $\vec{u}$-column degrees of a $\left(\mathbf{A},\vec{\sigma},\vec{u}\right)$-basis.
Then $\sum\vec{t}~\le~\sum\vec{s}+\sum\vec{\sigma}$%
\begin{comment}
 and $\max_{i}\left(\vec{t}_{i}-\vec{s}_{i}\right)\le\sigma$
\end{comment}
\textup{}%
\begin{comment}
need to permute the columns to put the pivots on the diagonal.
\end{comment}
. \end{lem}
\begin{proof}
\begin{comment}
For example, for a input matrix with two rows, instead of increasing
the order of both rows to $\left[1,1\right]$ at once, we can work
on the second row first to increase the order to $\left[0,1\right]$,
and then compute to order $\left[1,1\right]$.  
\end{comment}
The sum of the $\vec{s}$-column degrees is $\sum\vec{s}$ at order
$\left[0,\dots,0\right]$, since the identity matrix is a $\left(\mathbf{A},\left[0,\dots,0\right],\vec{s}\right)$-basis.
This sum increases by $1$ for each order increase of each row. The
total number of order increases required for all rows is at most $\sum\vec{\sigma}$.
Note that from \prettyref{thm:combineOrderBases}, we can work with
just one row at a time to increase its order in the order basis computation. 
\end{proof}
The following lemma extends \prettyref{thm:boundOfSumOfShiftedDegreesOfKernelBasis}
to give a bound based on the shifted column degrees or shifted row
degrees, instead of just the column degrees of the input matrix.
\begin{lem}
\label{lem:generalNullspaceBasisDegreeBound}If $\mathbf{A}\in\mathbb{K}^{m\times n}[x]$
has $\rdeg_{\vec{u}}\mathbf{A}\le\vec{v}$ or equivalently $\cdeg_{-\vec{v}}\mathbf{A}\le-\vec{u}$,
then any $(\mathbf{A},-\vec{u})$-kernel basis has $-\vec{u}$-column
degrees bounded by $\sum\vec{v}-\sum\vec{u}$. \end{lem}
\begin{proof}
Let $\mathbf{P}=\left[\mathbf{B},\bar{\mathbf{B}}\right]$ be a $\left(\mathbf{A},\vec{v}+\left[\sigma,\dots,\sigma\right],-\vec{u}\right)$-basis
containing a kernel basis, $\mathbf{B}$, of $\mathbf{A}$. Then $\sum\cdeg_{-\vec{u}}\mathbf{P}$
is at least $m\sigma+\sum\vec{v}-\sum\vec{u}$. We also know that
$\sum\cdeg_{-\vec{u}}\bar{\mathbf{B}}\ge\sum\cdeg_{-\vec{v}}\mathbf{A}\bar{\mathbf{B}}$,
but $\cdeg\mathbf{A}\bar{\mathbf{B}}\ge\vec{v}+\left[\sigma,\dots,\sigma\right]$
or $\sum\cdeg_{-\vec{v}}\mathbf{A}\bar{\mathbf{B}}\ge m\sigma$, therefore
$\sum\cdeg_{-\vec{u}}\bar{\mathbf{B}}\ge m\sigma$. It follows that
$\sum\cdeg_{-\vec{u}}\mathbf{B}\le m\sigma+\sum\vec{v}-\sum\vec{u}-m\sigma=\sum\vec{v}-\sum\vec{u}$.
\end{proof}
When the matrix $\mathbf{A}$ is also a $\left(\mathbf{B}^{T},\vec{u}\right)$-kernel
basis, as in our case, the bound in fact becomes tight.
\begin{lem}
\label{lem:mutualMinimalNullspaceBasisDegrees}Let $\mathbf{A}\in\mathbb{K}^{m\times n}[x]$
and $\mathbf{B}\in\mathbb{K}^{n\times(n-m)}\left[x\right]$. If $\mathbf{B}$
is a $(\mathbf{A},-\vec{u})$-kernel basis with $\cdeg_{-\vec{u}}\mathbf{B}=\vec{w}$
and $\mathbf{A}^{T}$ is a $\left(\mathbf{B}^{T},\vec{u}\right)$-kernel
basis with $\rdeg_{\vec{u}}\mathbf{A}=\vec{v}$, then $\sum\vec{w}=\sum\vec{v}-\sum\vec{u}$.\end{lem}
\begin{proof}
This follows from \prettyref{lem:generalNullspaceBasisDegreeBound},
which gives $\sum\vec{w}\le\sum\vec{v}-\sum\vec{u}$ and also $\sum\vec{v}\le\sum\vec{w}+\sum\vec{u}$
in the reverse direction.
\end{proof}
From \prettyref{lem:nullspaceBasisInOrderBasis}, we know that any
$\left(\mathbf{M}^{T},\vec{b}+1,-\vec{s}\right)$-basis contains a
$\left(\mathbf{M}^{T},-\vec{s}\right)$-kernel basis whose $-\vec{s}$-column
degrees bounded by 0. The following lemma shows that the remaining
part of the $\left(\mathbf{M}^{T},\vec{b}+1,-\vec{s}\right)$-basis
has degrees bounded by 1.
\begin{lem}
\label{lem:nullspaceOrderbasisDegree}Let $\mathbf{F}^{r}=\rowRev\left(\mathbf{F},-\vec{s},0\right)$
and $\mathbf{M}$ be a $(\mathbf{F}^{r},\vec{s})$-kernel basis with
$\cdeg_{\vec{s}}\mathbf{M}=\vec{b}$ as before. Let $\mathbf{P}$
be a $\left(\mathbf{M}^{T},\vec{b}+1,-\vec{s}\right)$-basis. Then
$\cdeg_{-\vec{b}-1}\mathbf{M}^{T}\mathbf{P}_{2}=\left[0,\dots,0\right]$
and $\cdeg_{-\vec{s}}\mathbf{P}_{2}=\left[1,\dots,1\right]$.\end{lem}
\begin{proof}
We already know that $\mathbf{P}$ contains a $\left(\mathbf{M}^{T},-\vec{s}\right)$-kernel
basis. Let this kernel basis be $\mathbf{P}_{1}$ in $\mathbf{P}=\left[\mathbf{P}_{1},\mathbf{P}_{2}\right]$.
We know that $\sum\cdeg_{-\vec{s}}\mathbf{P}=-\sum\vec{s}+\sum\vec{b}+n-m$
and for the kernel basis $\mathbf{P}_{1}$ in $\mathbf{P}$, we know
$\sum\cdeg_{-\vec{s}}\mathbf{P}_{1}=\sum\vec{b}-\sum\vec{s}$ from
\prettyref{lem:mutualMinimalNullspaceBasisDegrees}. Therefore, $\sum\cdeg_{-\vec{s}}\mathbf{P}_{2}=n-m$.
It follows that $\sum\cdeg_{-\vec{b}}\mathbf{M}^{T}\mathbf{P}_{2}\le\sum\cdeg_{-\vec{s}}\mathbf{P}_{2}=n-m$,
or $\sum\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}=0$.
But since $\mathbf{P}_{2}$ is nonzero and has order $\left(\mathbf{F},\vec{b}+\left[1,\dots,1\right]\right)$,
we have $\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}\ge\left[0,\dots,0\right]$,
implying $\sum\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}\ge0$.
It follows that $\sum\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}=0$,
hence $\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}=\left[0,\dots,0\right]$
or $\cdeg_{-\vec{b}}\mathbf{M}^{T}\mathbf{P}_{2}=\left[1,\dots,1\right]$.
Combining this with $\sum\cdeg_{-\vec{b}}\mathbf{M}^{T}\mathbf{P}_{2}\le\sum\cdeg_{-\vec{s}}\mathbf{P}_{2}=n-m$
we then get $\cdeg_{-\vec{s}}\mathbf{P}_{2}=\left[1,\dots,1\right]$.
\end{proof}
\begin{comment}
\begin{lem}
Let $\mathbf{F}^{r}=\rowRev\left(\mathbf{F},-\vec{s},0\right)$ and
$\mathbf{M}$ be a $(\mathbf{F}^{r},\vec{s})$-kernel basis with $\cdeg_{\vec{s}}\mathbf{M}=\vec{b}$
as before. Let $\mathbf{P}$ be a $\left(\mathbf{M}^{T},\vec{b}+\left[1,\dots,1\right],-\vec{s}\right)$-basis.
Then $\mathbf{M}^{T}\mathbf{P}$ has row degrees $\vec{b}+[1,\dots,1]$.
In other words, the row degrees are the same as the order.\end{lem}
\begin{proof}
From \prettyref{lem:nullspaceBasisOrderBasisContainsNullspaceBasis},
$\mathbf{P}$ contains a $\left(\mathbf{M}^{T},-\vec{s}\right)$-kernel
basis. Let this kernel basis be $\mathbf{P}_{1}$ in $\mathbf{P}=\left[\mathbf{P}_{1},\mathbf{P}_{2}\right]$.
We know that $\sum\cdeg_{-\vec{s}}\mathbf{P}=-\sum\vec{s}+\sum\vec{b}+n-m$
and for the kernel basis $\mathbf{P}_{1}$ in $\mathbf{P}$, we know
$\sum\cdeg_{-\vec{s}}\mathbf{P}_{1}=\sum\vec{b}-\sum\vec{s}$. Therefore,
$\sum\cdeg_{-\vec{s}}\mathbf{P}_{2}=n-m$. It follows that $\sum\cdeg_{-\vec{b}}\mathbf{M}^{T}\mathbf{P}_{2}\le\sum\cdeg_{-\vec{s}}\mathbf{P}_{2}=n-m$,
or $\sum\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}=0$.
But since $\mathbf{P}_{2}$ is nonzero and has order $\left(\mathbf{F},\vec{b}+\left[1,\dots,1\right]\right)$,
we have $\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}\ge\left[0,\dots,0\right]$,
implying $\sum\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}\ge0$.
It follows that $\sum\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}=0$,
hence $\cdeg_{-\vec{b}-\left[1,\dots,1\right]}\mathbf{M}^{T}\mathbf{P}_{2}=\left[0,\dots,0\right]$. 

Since 

or equivalently, $\rdeg\mathbf{M}^{T}\mathbf{P}_{2}=\vec{b}+\left[1,\dots,1\right]$.

We know that $\mathbf{M}^{T}$ has an identity matrix GCD, hence by
\prettyref{lem:CoprimeFullRankConstantCoefficientMatrix} $\mathbf{M}^{T}\mod x$
is full-rank, and by \prettyref{cor:fullRankConstantCoefficientMatrix}
$x^{-\vec{b}-[1,\dots,]}\mathbf{M}^{T}\mathbf{P}\mod x$ is also full-rank.
Now since 
\begin{eqnarray*}
 &  & \rowRev(\mathbf{M}^{T},\vec{s},\vec{b})\colRev(\mathbf{P},-\vec{s},\cdeg_{-\vec{s}}\mathbf{P})\\
 & = & x^{\vec{b}}\left(\mathbf{M}^{T}(1/x)\right)x^{-\vec{s}}x^{\vec{s}}\left(\mathbf{P}(1/x)\right)x^{\cdeg_{-\vec{s}}\mathbf{P}}\\
 & = & x^{\vec{b}}\left(\mathbf{M}^{T}(1/x)\right)\left(\mathbf{P}(1/x)\right)x^{\cdeg_{-\vec{s}}\mathbf{P}}\\
 & = & x^{\vec{b}}\left(\mathbf{M}^{T}\mathbf{P}(1/x)\right)x^{\cdeg_{-\vec{s}}\mathbf{P}}
\end{eqnarray*}


We know that $\mathbf{P}$ contains a $(\mathbf{M}^{T},-\vec{s})$-kernel
basis since any $(\mathbf{M}^{T},-\vec{s})$-kernel basis has $-\vec{s}$-column
degrees bounded by that of $\mathbf{F}^{rT}$, which is the same as
that of $\mathbf{F}^{T}$ and no more than $0$.\end{proof}
\end{comment}


We are now ready to look at the algorithm for computing a $\left(\mathbf{M}^{T},\vec{b}+\left[1,\dots,1\right],-\vec{s}\right)$-basis,
given in \prettyref{alg:unimodularCompletion}.  We follow the same
process as in \prettyref{sec:computeRightFactor}. We assume without
loss of generality that the rows of $\mathbf{M}^{T}$ are arranged
in decreasing $\vec{s}$-row degrees. We divide $\mathbf{M}^{T}$
into $\log k$ row blocks according to the $\vec{s}$-row degrees
of its rows. Let 
\[
\mathbf{M}^{T}=\left[\mathbf{M}_{1}^{T},\mathbf{M}_{2}^{T},\cdots,\mathbf{M}_{\log k-1}^{T},\mathbf{M}_{\log k}^{T}\right]
\]
 with $\mathbf{M}_{\log k},\mathbf{M}_{\log k-1},\cdots,\mathbf{M}_{2},\mathbf{M}_{1}$
having $\vec{s}$-row degrees in the range $\left[0,2\xi/k\right]$,
$(2\xi/k,4\xi/k],$ $(4\xi/k,8\xi/k],\ ...,$ $(\xi/4,\xi/2],$ $(\xi/2,\xi].$Let
$\vec{\sigma}_{i}=\left[\xi/2^{i-1}+1,\dots,\xi/2^{i-1}+1\right]$
with the same dimension as the row dimension of $\mathbf{M}_{i}$.
Let $\vec{\sigma}=\left[\vec{\sigma}_{\log k},\vec{\sigma}_{\log k-1},\dots,\vec{\sigma}_{1}\right]$
be the order in the order basis computation. For simplicity, instead
of using $\mathbf{M}^{T}$ as the input matrix, we use 
\begin{eqnarray*}
\hat{\mathbf{M}} & =\begin{bmatrix}\hat{\mathbf{M}}_{1}\\
\vdots\\
\hat{\mathbf{M}}_{\log k}
\end{bmatrix}= & x^{\vec{\sigma}-\vec{b}-1}\begin{bmatrix}\mathbf{M}_{1}\\
\vdots\\
\mathbf{M}_{\log k}
\end{bmatrix}=x^{\vec{\sigma}-\vec{b}-1}\mathbf{M}
\end{eqnarray*}
 instead, so that a $\left(\hat{\mathbf{M}},\vec{\sigma},-\vec{s}\right)$-basis
is a $\left(\mathbf{M},\vec{b}+1,-\vec{s}\right)$-basis.



\begin{comment}
kjh
\begin{lem}
If $\mathbf{A}$ is $\vec{u}$-row reduced and has an identity matrix
GCD, and if $\mathbf{P}$ is a $\left(\mathbf{A},\vec{v},-\vec{u}\right)$-basis
that contains a complete kernel basis of $\mathbf{A}$, then $\det\mathbf{P}=x^{\sum\vec{v}}$
or equivalently, $\sum\cdeg_{-\vec{u}}\mathbf{P}=\sum\vec{v}-\sum\vec{u}$.\end{lem}
\begin{proof}
In general, we have $\det\mathbf{P}\le x^{\sum\vec{v}}$, since each
increase of the $-\vec{u}$ column degree of some column of $\mathbf{P}$
also increases the order of at least one row by one. Therefore, to
show the equality holds in our special case here, we just need to
show that $\det\mathbf{P}\nless x^{\sum\vec{v}}$. If $\mathbf{A}=\left[I,0\right]$,
then it is not difficult to see that the lemma is true, since $\diag\left(\left[x^{\vec{v}},I\right]\right)$
is a $\left(\mathbf{A},\vec{v},-\vec{u}\right)$-basis.

Note that there is a $\left(\mathbf{A},\vec{v},-\vec{u}\right)$-basis
$\mathbf{Q}$ such that $\mathbf{A}\mathbf{Q}=\left[x^{\vec{v}},0\right]$.
\end{proof}
But the equality hold in the special case here.
\begin{lem}
Let $\mathbf{P}=\left[\mathbf{P}_{1},\mathbf{P}_{2}\right]$ be a
$\left(\mathbf{M}^{T},\vec{b}+\left[1,\dots,1\right],-\vec{s}\right)$-basis
as before. 
\end{lem}
The only component we need to consider 

To compute a unimodular completion of $\mathbf{F}$ with column degrees
bounded component-wise by $\vec{s}$ (or equivalently, $-\vec{s}$-row
degrees bounded by 0), let us first compute a $\vec{s}$-minimal kernel
basis $\mathbf{N}$ of $\mathbf{F}$. Let $\vec{b}$ be the $\vec{s}$-column
degrees of $\mathbf{N}$. Let $\bar{\mathbf{N}}=\colRev\left(\mathbf{N},\vec{s},\vec{b}\right)^{T}$.
We then compute a $\left(\bar{\mathbf{N}},-\vec{s},\vec{b}+1\right)$-basis
$\bar{\mathbf{P}}$, which would contain a complete kernel basis $\bar{\mathbf{F}}$
of $\bar{\mathbf{N}}$ since the row degrees of $\bar{\mathbf{N}}\bar{\mathbf{F}}$
are bounded by the corresponding entries of $\vec{b}$ and $\order(\bar{\mathbf{N}},\bar{\mathbf{P}})$
is greater than $\vec{b}$ component-wise . Note that the $-\vec{s}$-column
degrees of $\bar{\mathbf{P}}$ are bounded by $\left[1,\dots,1\right]$
since the $-\vec{s}$-row degrees of $\bar{\mathbf{F}}^{T}$ are bounded
by that of $\mathbf{F}$, which are bounded by 0.
\begin{lem}
Given $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$ with $\cdeg\mathbf{F}\le\vec{s}$
(or equivalently, $\rdeg_{-\vec{s}}\mathbf{F}\le0$). If $\mathbf{N}$
is a $\left(\mathbf{F},\infty,\vec{s}\right)$-basis with $\cdeg_{\vec{s}}\mathbf{N}=\vec{b}$, \end{lem}
\end{comment}


We now do a series of order basis computations in order to compute
a unimodular completion of $\mathbf{F}$ based on \prettyref{lem:unimodularComputationByRows}.

Let $\vec{s}_{1}=\vec{s}$. First we compute an $\left(\hat{\mathbf{M}}_{1},\vec{\sigma}_{1},-\vec{s}_{1}\right)$-basis
$\mathbf{P}_{1}=\left[\mathbf{N}_{1},\mathbf{Q}_{1}\right]$, where
$\mathbf{N}_{1}$ is a $\left(\hat{\mathbf{M}}_{1},-\vec{s}_{1}\right)$-kernel
basis%
\begin{comment}
 with $\cdeg_{-\vec{s}_{1}}\mathbf{N}_{1}\le0$
\end{comment}
. This computation can be done using \prettyref{alg:umab} with a
cost of $O^{\sim}\left(n^{\omega}d\right)$, where $d=\xi/n$. 

Let $\tilde{\mathbf{N}}_{1}=\mathbf{N}_{1}$. Let $\vec{s}_{2}=-\cdeg_{-\vec{s}}\mathbf{N}_{1}$
and $\vec{t}_{2}=-\cdeg_{-\vec{s}}\mathbf{Q}_{1}$. %
\begin{comment}
Note that $-\vec{s}_{1}\le-[\vec{s}_{2},\vec{t}_{2}]\le\left[0,\dots,0,1,\dots1\right]$
component-wise, since $\mathbf{P}_{1}$ has lower order than any $\left(\mathbf{M}^{T},\vec{b}+\left[1,\dots,1\right],-\vec{s}\right)$-basis
$\mathbf{P}$ hence generates $\mathbf{P}$. Therefore, $\cdeg_{-\vec{s}}\mathbf{P}_{1}\le\cdeg_{-\vec{s}}\mathbf{P}\le\left[0,\dots,0,1,\dots1\right]$. 
\end{comment}
{} We then compute an $\left(\hat{\mathbf{M}}_{2}\tilde{\mathbf{N}}_{1},\vec{\sigma}_{2},-\vec{s}_{2}\right)$-basis
$\mathbf{P}_{2}=\left[\mathbf{N}_{2},\mathbf{Q}_{2}\right]$ with
$\vec{s}_{3}=-\cdeg_{-\vec{s}_{2}}\mathbf{N}_{2}$ and $\vec{t}_{3}=-\cdeg_{-\vec{s}_{2}}\mathbf{Q}_{2}$.
Let $\tilde{\mathbf{N}}_{2}=\tilde{\mathbf{N}}_{1}\mathbf{N}_{2}$
 %
\begin{comment}
Let $\mathbf{R}_{1}=\left[\mathbf{N}_{1}\mathbf{Q}_{2},\mathbf{Q}_{1}\right]$
and $\mathbf{R}_{1}^{r}=\colRev\left(\mathbf{R}_{1},-\vec{s},\cdeg_{-\vec{s}}\mathbf{R}_{1}\right)$.
Then from \prettyref{lem:unimodularComputationByRows} we know $\left[\mathbf{F}^{T},\mathbf{R}_{1}^{r}\right]$
is a unimodular matrix.
\end{comment}


Continue this process, at step $i$, we compute an $\left(\hat{\mathbf{M}}_{i}\tilde{\mathbf{N}}_{i-1},\vec{\sigma}_{i},-\vec{s}_{i}\right)$-basis
$\mathbf{P}_{i}=\left[\mathbf{N}_{i},\mathbf{Q}_{i}\right]$. Let
$\tilde{\mathbf{N}}_{i}=\prod_{j=1}^{i}\mathbf{N}_{i}=\tilde{\mathbf{N}}_{i-1}\mathbf{N}_{i}$.
Note that $\tilde{\mathbf{N}}_{\log k}$ is a $\left(\mathbf{M},-\vec{s}\right)$-kernel
basis. Let $\mathbf{R}=\left[\mathbf{Q}_{1},\tilde{\mathbf{N}}_{1}\mathbf{Q}_{2},\dots,\tilde{\mathbf{N}}_{\log k-2}\mathbf{Q}_{\log k-1},\tilde{\mathbf{N}}_{\log k-1}\mathbf{Q}_{\log k}\right]$,
and $\mathbf{R}^{r}=\colRev\left(\mathbf{R},-\vec{s},\cdeg_{-\vec{s}}\mathbf{R}\right)$,
then from \prettyref{lem:unimodularComputationByRows} we can conclude
that $\left[\mathbf{F}^{T},\mathbf{R}^{r}\right]$ is a unimodular
matrix. 

\input{algorithmUnimodularCompletion.tex}

We still need to check the cost of the multiplications $\hat{\mathbf{M}}_{i}\tilde{\mathbf{N}}_{i-1}$,
$\tilde{\mathbf{N}}_{i-1}\mathbf{N}_{i}$, and $\tilde{\mathbf{N}}_{i-1}\mathbf{Q}_{i}$. 
\begin{lem}
The multiplications $\hat{\mathbf{M}}_{i}\tilde{\mathbf{N}}_{i-1}$
can be done with a cost of $O^{\sim}\left(n^{\omega}d\right)$.\end{lem}
\begin{proof}
The dimension of $\hat{\mathbf{M}}_{i}$ is bounded by $2^{i-1}\times n$
and $\sum\rdeg_{\vec{s}}\hat{\mathbf{M}}_{i}\le2^{i-1}\cdot\xi/2^{i-1}=\xi$.
We also have $\cdeg_{-\vec{s}}\tilde{\mathbf{N}}_{i-1}\le0$, or equivalently,
$\rdeg\tilde{\mathbf{N}}_{i-1}\le\vec{s}$. We can now use \prettyref{thm:multiplyUnbalancedMatrices}
to multiply $\tilde{\mathbf{N}}_{i-1}^{T}$ and $\hat{\mathbf{M}}_{i}^{T}$
with a cost of $O^{\sim}\left(n^{\omega-1}\xi\right)=O^{\sim}\left(n^{\omega}d\right)$.\end{proof}
\begin{lem}
The multiplication $\tilde{\mathbf{N}}_{i-1}\mathbf{N}_{i}$ can be
done with a cost of $O^{\sim}\left(n^{\omega}d\right)$.\end{lem}
\begin{proof}
We know $\cdeg_{-\vec{s}}\tilde{\mathbf{N}}_{i-1}=-\vec{s}_{i}$,
and $\cdeg_{-\vec{s}_{i}}\mathbf{N}_{i}=-\vec{s}_{i+1}\le0.$ In other
words, $\rdeg\mathbf{N}_{i}\le\vec{s}_{i}$, and $\rdeg_{\vec{s}_{i}}\tilde{\mathbf{N}}_{i-1}\le\vec{s}$,
hence we can again use \prettyref{thm:multiplyUnbalancedMatrices}
to multiply $\mathbf{N}_{i}^{T}$ and $\tilde{\mathbf{N}}_{i-1}^{T}$
with a cost of $O^{\sim}\left(n^{\omega}d\right)$.\end{proof}
\begin{lem}
The multiplication $\tilde{\mathbf{N}}_{i-1}\mathbf{Q}_{i}$ can be
done with a cost of $O^{\sim}\left(n^{\omega}d\right)$.\end{lem}
\begin{proof}
We know $\cdeg_{-\vec{s}_{i}}\mathbf{Q}_{i}\le\max\cdeg_{\vec{s}}\mathbf{P}=1$,
or equivalently, $\rdeg\mathbf{Q}_{i}\le\vec{s}_{i}+\left[1,\dots,1\right]$.
But we also know that this $\mathbf{Q}_{i}$ from the order basis
computation has a factor $xI$. Therefore, $\rdeg\left(\mathbf{Q}_{i}/x\right)\le\vec{s}_{i}$.
In addition, $\rdeg_{\vec{s}_{i}}\tilde{\mathbf{N}}_{i-1}\le\vec{s}$
as before. So we can again use \prettyref{thm:multiplyUnbalancedMatrices}
to multiply $\mathbf{Q}_{i}^{T}$ and $\tilde{\mathbf{N}}_{i-1}^{T}$
with a cost of $O^{\sim}\left(n^{\omega-1}\xi\right)$.\end{proof}
\begin{thm}
A unimodular completion of $\mathbf{F}$ can be computed with a cost
of $O^{\sim}\left(n^{\omega-1}\xi\right)$ field operations.\end{thm}

